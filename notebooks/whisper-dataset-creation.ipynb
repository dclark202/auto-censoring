{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "84d34493",
   "metadata": {},
   "source": [
    "This notebook contains the necessary code to create a Whisper dataset from raw audio files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27a04fa6",
   "metadata": {},
   "source": [
    "-------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "947ff1e0",
   "metadata": {},
   "source": [
    "Define Whisper processor\n",
    "- openai/whisper-base is common\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d50b3021",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import WhisperProcessor\n",
    "import torch\n",
    "\n",
    "model_name = \"openai/whisper-base\"\n",
    "language = \"english\" \n",
    "task = \"transcribe\" \n",
    "\n",
    "processor = WhisperProcessor.from_pretrained(model_name, language=language, task=task)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed2031ef",
   "metadata": {},
   "source": [
    "Metadata file must be created before this can be run. Must contain\n",
    "- Audio file paths\n",
    "- Transcript (full transcript, not word-level)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e0e600",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, Audio, DatasetDict\n",
    "\n",
    "# Chunked wav dataset\n",
    "\n",
    "dataset_path = \"T:\\\\dl-project\\\\DALI-chunks-lines\"\n",
    "# dataset_path = \"C:\\\\Users\\\\dacla\\\\Documents\\\\chunks\" #ela's dataset\n",
    "\n",
    "## Metadata file is crucial\n",
    "raw_dataset = load_dataset(\"csv\", data_files=\"metadata-wer0-lines.csv\", split='train')\n",
    "print(\"Full dataset\\n\", raw_dataset)\n",
    "\n",
    "# Make a train/test split at this point !\n",
    "raw_dataset = raw_dataset.train_test_split(test_size=0.1, shuffle=True, seed=555)\n",
    "\n",
    "print(\"----------\")\n",
    "print(\"Split dataset\\n\", raw_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9733320e",
   "metadata": {},
   "source": [
    "Prepare the dataset using the Whisper processor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a2c1ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "\n",
    "def prepare_dataset(batch):\n",
    "    # Load and resample audio data\n",
    "    audio_paths = [f\"{dataset_path}\\\\{fname}\" for fname in batch['filename']]\n",
    "    audio_arrays = [librosa.load(path, sr=16000)[0] for path in audio_paths]\n",
    "    \n",
    "    # Compute log-Mel input features from the audio\n",
    "    batch['input_features'] = processor.feature_extractor(audio_arrays, \n",
    "                                                          sampling_rate=16000,\n",
    "                                                          return_tensors='pt').input_features\n",
    "\n",
    "    # Encode the transcriptions to label ids\n",
    "    batch['labels'] = processor.tokenizer(batch['words'],\n",
    "                                           max_length=processor.tokenizer.model_max_length,\n",
    "                                           truncation=True,\n",
    "                                           #padding='do_not_pad', # no padding is the default. Pad in the collator\n",
    "                                           return_tensors=None).input_ids\n",
    "\n",
    "\n",
    "    return batch\n",
    "\n",
    "# Apply the function to the entire dataset\n",
    "processed_dataset = raw_dataset.map(prepare_dataset, \n",
    "                                    batched=True, \n",
    "                                    batch_size=8, \n",
    "                                    remove_columns=raw_dataset.column_names[\"train\"])\n",
    "\n",
    "# And save to the disc\n",
    "processed_dataset.save_to_disk('wer0-dataset-fixed-padding')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4a6d744",
   "metadata": {},
   "source": [
    "The following does a check to make sure the inputs are formatted correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3ef7520",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming processed_dataset is ready\n",
    "print(\"\\n--- Verifying processed_dataset labels after map ---\")\n",
    "# Get a sample from the processed_dataset (e.g., the first 5 samples)\n",
    "sample_data = processed_dataset[\"train\"].select(range(min(5, len(processed_dataset[\"train\"]))))\n",
    "\n",
    "processor_instance = processor # Use the processor you defined earlier\n",
    "\n",
    "for i, sample in enumerate(sample_data):\n",
    "    labels = sample[\"labels\"] # These are the token IDs from prepare_dataset\n",
    "\n",
    "    # Ensure labels is a list (if it came from prepare_dataset's list of lists)\n",
    "    if isinstance(labels, torch.Tensor):\n",
    "        labels_list = labels.tolist()\n",
    "    else: # It's likely a list of lists if batched=True in map\n",
    "        # If it's a single sample, it might just be a list\n",
    "        labels_list = labels \n",
    "        if isinstance(labels_list[0], list): # If it's a list of lists (from batched=True)\n",
    "            labels_list = labels_list[0] # Take the first one if you expect single samples here\n",
    "\n",
    "    decoded_full = processor_instance.tokenizer.decode(labels_list, skip_special_tokens=False)\n",
    "    decoded_clean = processor_instance.tokenizer.decode(labels_list, skip_special_tokens=True)\n",
    "    \n",
    "    eos_id = processor_instance.tokenizer.eos_token_id\n",
    "\n",
    "    print(f\"\\nSample {i+1}:\")\n",
    "    print(f\"  Raw Labels IDs: {labels_list}\")\n",
    "    print(f\"  Decoded (with special tokens): '{decoded_full}'\")\n",
    "    print(f\"  Decoded (clean text): '{decoded_clean}'\")\n",
    "    \n",
    "    if labels_list and labels_list[-1] == eos_id:\n",
    "        print(f\"  Ends with EOS token ({eos_id}): YES\")\n",
    "    else:\n",
    "        print(f\"  Ends with EOS token ({eos_id}): NO - CRITICAL ISSUE AT prepare_dataset!\")\n",
    "        if labels_list:\n",
    "            print(f\"    Last token: {labels_list[-1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "839f748f",
   "metadata": {},
   "source": [
    "---------------------------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "auto-censoring",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
