{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "27a04fa6",
   "metadata": {},
   "source": [
    "-------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "947ff1e0",
   "metadata": {},
   "source": [
    "Prepare the dataset with the Whisper processor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d50b3021",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import WhisperProcessor\n",
    "\n",
    "model_name = \"openai/whisper-medium.en\"\n",
    "language = \"english\" # Change to your dataset's language\n",
    "task = \"transcribe\" # Use \"translate\" if you're translating to English\n",
    "\n",
    "processor = WhisperProcessor.from_pretrained(model_name, language=language, task=task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f2e0e600",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full dataset Dataset({\n",
      "    features: ['transcript', 'file-wav'],\n",
      "    num_rows: 29656\n",
      "})\n",
      "\n",
      "Split dataset DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['transcript', 'file-wav'],\n",
      "        num_rows: 23724\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['transcript', 'file-wav'],\n",
      "        num_rows: 5932\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, Audio, DatasetDict\n",
    "\n",
    "# Chunked wav dataset\n",
    "dataset_path = \"C:\\\\Users\\\\dacla\\\\Documents\\\\DALI-chunks-wav\"\n",
    "\n",
    "raw_dataset = load_dataset(\"csv\", data_files=\"metadata-wav.csv\", split='train')\n",
    "print(\"Full dataset\", raw_dataset)\n",
    "\n",
    "# Make a train/test split at this point !\n",
    "raw_dataset = raw_dataset.train_test_split(test_size=0.2, shuffle=True, seed=555)\n",
    "print(\"\\nSplit dataset\", raw_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "5a2c1ea0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f74e468a8919420e9b3a164e45792184",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/23724 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f80521603ffc4f44ba7719b5c4b84d8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5932 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import librosa\n",
    "\n",
    "def prepare_dataset(batch):\n",
    "    # Load and resample audio data\n",
    "    audio_paths = [f\"{dataset_path}\\\\{fname}\" for fname in batch['file-wav']]\n",
    "    audio_arrays = [librosa.load(path, sr=16000)[0] for path in audio_paths]\n",
    "    \n",
    "    # Compute log-Mel input features from the audio\n",
    "    batch[\"input_features\"] = processor.feature_extractor(audio_arrays, sampling_rate=16000).input_features\n",
    "\n",
    "    # Encode the transcriptions to label ids\n",
    "    labels = processor.tokenizer(batch[\"transcript\"]).input_ids\n",
    "    batch[\"labels\"] = [[label if label != processor.tokenizer.pad_token_id else -100 for label in T] for T in labels]\n",
    "\n",
    "    return batch\n",
    "\n",
    "# Apply the function to the entire dataset\n",
    "processed_dataset = raw_dataset.map(\n",
    "    prepare_dataset,\n",
    "    batched=True,\n",
    "    batch_size=8,\n",
    "    remove_columns=raw_dataset.column_names[\"train\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "5f96e91c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f00c9f9c39540f48ca754f99bb8a949",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/46 shards):   0%|          | 0/23724 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d9b1042742f40cc984eab390a47ba37",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/12 shards):   0%|          | 0/5932 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Save dataset to disc\n",
    "processed_dataset.save_to_disk('dataset_whisper')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b55e1da8",
   "metadata": {},
   "source": [
    "Start from here if retraining\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "85931646",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import WhisperProcessor\n",
    "\n",
    "model_name = \"openai/whisper-medium.en\"\n",
    "language = \"english\" # Change to your dataset's language\n",
    "task = \"transcribe\" # Use \"translate\" if you're translating to English\n",
    "\n",
    "processor = WhisperProcessor.from_pretrained(model_name, language=language, task=task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "18e62126",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForSeq2Seq\n",
    "import evaluate\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, Dict, List, Union\n",
    "import torch\n",
    "\n",
    "# --- Data Collator ---\n",
    "@dataclass\n",
    "class DataCollatorSpeechSeq2SeqWithPadding:\n",
    "    processor: Any\n",
    "    padding: Union[bool, str] = True\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
    "        # Split inputs and labels since they have to be of different lengths and need different padding methods.\n",
    "        # \"input_features\" for Whisper-based models (vs. \"input_values\" for wav2vec...)\n",
    "        input_features = [{\"input_features\": feature[\"input_features\"]} for feature in features]\n",
    "        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
    "\n",
    "        batch = self.processor.feature_extractor.pad(input_features, \n",
    "                                                     return_tensors=\"pt\",\n",
    "                                                     return_attention_mask=True)\n",
    "        \n",
    "        labels_batch = self.processor.tokenizer.pad(label_features, return_tensors=\"pt\")\n",
    "\n",
    "        # Replace padding with -100 to ignore loss correctly\n",
    "        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
    "\n",
    "        batch[\"labels\"] = labels\n",
    "        return batch\n",
    "\n",
    "data_collator = DataCollatorSpeechSeq2SeqWithPadding(processor=processor, padding=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54a24f62",
   "metadata": {},
   "source": [
    "Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "179912e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model openai/whisper-medium.en loaded on cuda\n"
     ]
    }
   ],
   "source": [
    "from transformers import WhisperForConditionalGeneration\n",
    "\n",
    "model = WhisperForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "# This is necessary for the model to work correctly with the Trainer\n",
    "model.config.forced_decoder_ids = None\n",
    "model.config.suppress_tokens = []\n",
    "\n",
    "# send to the appropriate device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model.to(device)\n",
    "\n",
    "print(f'Model {model_name} loaded on {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27752e5c",
   "metadata": {},
   "source": [
    "Downsample if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c240d898",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4767e09ccbfe4ba98c0985c0cc2424a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset from disk:   0%|          | 0/46 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Full Prepared Dataset ---\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['input_features', 'labels'],\n",
      "        num_rows: 23724\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['input_features', 'labels'],\n",
      "        num_rows: 5932\n",
      "    })\n",
      "})\n",
      "\n",
      "--- Sampled (0.1%) Dataset ---\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['input_features', 'labels'],\n",
      "        num_rows: 23\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['input_features', 'labels'],\n",
      "        num_rows: 5\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_from_disk\n",
    "\n",
    "sample_percentage = 0.001\n",
    "\n",
    "# Load full prepared dataset\n",
    "prepared_dataset_path = 'dataset_whisper'\n",
    "prepared_datasets = load_from_disk(prepared_dataset_path)\n",
    "print(\"--- Full Prepared Dataset ---\")\n",
    "print(prepared_datasets)\n",
    "\n",
    "# Sample 1% from the training set\n",
    "train_split = prepared_datasets[\"train\"]\n",
    "sampled_train_split = train_split.train_test_split(train_size=sample_percentage, shuffle=True, seed=555)['train'] # We only want the 'train' part of this new split\n",
    "\n",
    "test_split = prepared_datasets[\"test\"]\n",
    "sampled_test_split = test_split.train_test_split(train_size=sample_percentage, shuffle=True, seed=555)['train'] \n",
    "\n",
    "# Overwrite the original splits with the sampled splits\n",
    "prepared_datasets['train'] = sampled_train_split\n",
    "prepared_datasets['test'] = sampled_test_split\n",
    "\n",
    "print(f\"\\n--- Sampled ({sample_percentage*100}%) Dataset ---\")\n",
    "print(prepared_datasets)\n",
    "\n",
    "# Now, use this smaller `prepared_datasets` object for the rest of your script\n",
    "# (creating DataLoaders, etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "443b335a",
   "metadata": {},
   "source": [
    "Training parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3d562d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from transformers import get_scheduler\n",
    "from torch.optim import AdamW\n",
    "\n",
    "# Training parameters\n",
    "learning_rate = .001\n",
    "train_batch_size = 2\n",
    "eval_batch_size = 4\n",
    "\n",
    "# Defined train and test DLs\n",
    "train_dataloader = DataLoader(\n",
    "    prepared_datasets[\"train\"],\n",
    "    shuffle=True,\n",
    "    collate_fn=data_collator,\n",
    "    batch_size=train_batch_size\n",
    ")\n",
    "\n",
    "eval_dataloader = DataLoader(\n",
    "    prepared_datasets[\"test\"],\n",
    "    collate_fn=data_collator,\n",
    "    batch_size=eval_batch_size\n",
    ")\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
    "scaler = torch.amp.GradScaler('cuda')\n",
    "wer_metric = evaluate.load(\"wer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbaf9c10",
   "metadata": {},
   "source": [
    "Main training cycle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "447f352c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training steps: 60\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1/5:   0%|          | 0/12 [00:00<?, ?it/s]Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.43.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n",
      "Training Epoch 1/5:  17%|█▋        | 2/12 [00:07<00:43,  4.32s/it]"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "num_train_epochs = 5\n",
    "num_warmup_steps = 0\n",
    "total_steps = len(train_dataloader) * num_train_epochs\n",
    "best_wer = float('inf')\n",
    "output_dir = \".\\\\whisper-ft\"\n",
    "\n",
    "print(f\"Total training steps: {total_steps}\")\n",
    "\n",
    "for epoch in range(num_train_epochs):\n",
    "    # --- TRAINING ---\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    \n",
    "    # Use tqdm for a progress bar\n",
    "    for step, batch in enumerate(tqdm(train_dataloader, desc=f\"Training Epoch {epoch + 1}/{num_train_epochs}\")):\n",
    "        # Move batch to device\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "\n",
    "        # Backward pass\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    avg_train_loss = train_loss / len(train_dataloader)\n",
    "    print(f\"Epoch {epoch + 1} | Average Training Loss: {avg_train_loss:.4f}\")\n",
    "\n",
    "    # --- EVALUATION ---\n",
    "    model.eval()\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "\n",
    "    # Use torch.no_grad() to save memory and computations\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(eval_dataloader, desc=\"Evaluating\"):\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            \n",
    "            # Generate predictions. This is different from the training forward pass.\n",
    "            generated_ids = model.generate(input_features=batch[\"input_features\"], \n",
    "                                           attention_mask=batch[\"attention_mask\"], \n",
    "                                           max_length=225)\n",
    "            \n",
    "            # Decode predictions\n",
    "            predictions = processor.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "            \n",
    "            # Decode labels, replacing -100 with pad token\n",
    "            labels = batch[\"labels\"].clone()\n",
    "            labels[labels == -100] = processor.tokenizer.pad_token_id\n",
    "            labels_str = processor.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "            all_predictions.extend(predictions)\n",
    "            all_labels.extend(labels_str)\n",
    "\n",
    "    # Compute WER\n",
    "    wer = wer_metric.compute(predictions=all_predictions, references=all_labels)\n",
    "    wer = round(100 * wer, 2)\n",
    "    \n",
    "    print(f\"Epoch {epoch + 1} | WER: {wer}%\")\n",
    "\n",
    "    # Save the model if it has the best WER so far\n",
    "    if wer < best_wer:\n",
    "        best_wer = wer\n",
    "        print(f\"New best WER: {best_wer}%. Saving model...\")\n",
    "        model.save_pretrained(output_dir)\n",
    "        processor.save_pretrained(output_dir)\n",
    "        print(f\"Model saved to {output_dir}\")\n",
    "\n",
    "print(\"\\n--- Training Complete ---\")\n",
    "print(f\"Best WER achieved: {best_wer}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a804e025",
   "metadata": {},
   "source": [
    "--------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e1ddab3",
   "metadata": {},
   "source": [
    "Testing a trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b0c9c7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = evaluate.load(\"wer\")\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    pred_ids = pred.predictions\n",
    "    label_ids = pred.label_ids\n",
    "\n",
    "    # Replace -100 with the pad token id\n",
    "    label_ids[label_ids == -100] = processor.tokenizer.pad_token_id\n",
    "\n",
    "    # Decode predictions and labels\n",
    "    pred_str = processor.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "    label_str = processor.batch_decode(label_ids, skip_special_tokens=True)\n",
    "\n",
    "    # Compute WER\n",
    "    wer = 100 * metric.compute(predictions=pred_str, references=label_str)\n",
    "    return {\"wer\": wer}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a622cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Load the fine-tuned model\n",
    "asr_pipeline = pipeline(\"automatic-speech-recognition\", model=output_dir, device=device)\n",
    "\n",
    "# Transcribe audio\n",
    "result = asr_pipeline(\"/path/to/some/new_audio.wav\")\n",
    "print(result[\"text\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
