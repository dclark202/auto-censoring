{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "27a04fa6",
   "metadata": {},
   "source": [
    "-------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "947ff1e0",
   "metadata": {},
   "source": [
    "Prepare the dataset with the Whisper processor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d50b3021",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import WhisperProcessor\n",
    "\n",
    "model_name = \"openai/whisper-medium.en\"\n",
    "language = \"english\" # Change to your dataset's language\n",
    "task = \"transcribe\" # Use \"translate\" if you're translating to English\n",
    "\n",
    "processor = WhisperProcessor.from_pretrained(model_name, language=language, task=task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f2e0e600",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full dataset Dataset({\n",
      "    features: ['transcript', 'file-wav'],\n",
      "    num_rows: 29656\n",
      "})\n",
      "\n",
      "Split dataset DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['transcript', 'file-wav'],\n",
      "        num_rows: 23724\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['transcript', 'file-wav'],\n",
      "        num_rows: 5932\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, Audio, DatasetDict\n",
    "\n",
    "# Chunked wav dataset\n",
    "dataset_path = \"C:\\\\Users\\\\dacla\\\\Documents\\\\DALI-chunks-wav\"\n",
    "\n",
    "raw_dataset = load_dataset(\"csv\", data_files=\"metadata-wav.csv\", split='train')\n",
    "print(\"Full dataset\", raw_dataset)\n",
    "\n",
    "# Make a train/test split at this point !\n",
    "raw_dataset = raw_dataset.train_test_split(test_size=0.2, shuffle=True, seed=555)\n",
    "print(\"\\nSplit dataset\", raw_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "5a2c1ea0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f74e468a8919420e9b3a164e45792184",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/23724 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f80521603ffc4f44ba7719b5c4b84d8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5932 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import librosa\n",
    "\n",
    "def prepare_dataset(batch):\n",
    "    # Load and resample audio data\n",
    "    audio_paths = [f\"{dataset_path}\\\\{fname}\" for fname in batch['file-wav']]\n",
    "    audio_arrays = [librosa.load(path, sr=16000)[0] for path in audio_paths]\n",
    "    \n",
    "    # Compute log-Mel input features from the audio\n",
    "    batch[\"input_features\"] = processor.feature_extractor(audio_arrays, sampling_rate=16000).input_features\n",
    "\n",
    "    # Encode the transcriptions to label ids\n",
    "    labels = processor.tokenizer(batch[\"transcript\"]).input_ids\n",
    "    batch[\"labels\"] = [[label if label != processor.tokenizer.pad_token_id else -100 for label in T] for T in labels]\n",
    "\n",
    "    return batch\n",
    "\n",
    "# Apply the function to the entire dataset\n",
    "processed_dataset = raw_dataset.map(\n",
    "    prepare_dataset,\n",
    "    batched=True,\n",
    "    batch_size=8,\n",
    "    remove_columns=raw_dataset.column_names[\"train\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "5f96e91c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f00c9f9c39540f48ca754f99bb8a949",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/46 shards):   0%|          | 0/23724 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d9b1042742f40cc984eab390a47ba37",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/12 shards):   0%|          | 0/5932 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Save dataset to disc\n",
    "processed_dataset.save_to_disk('dataset_whisper')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b55e1da8",
   "metadata": {},
   "source": [
    "Start from here if retraining\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "85931646",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import WhisperProcessor\n",
    "\n",
    "model_name = \"openai/whisper-medium.en\"\n",
    "language = \"english\" # Change to your dataset's language\n",
    "task = \"transcribe\" # Use \"translate\" if you're translating to English\n",
    "\n",
    "processor = WhisperProcessor.from_pretrained(model_name, language=language, task=task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "18e62126",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForSeq2Seq\n",
    "import evaluate\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, Dict, List, Union\n",
    "import torch\n",
    "\n",
    "# --- Data Collator ---\n",
    "@dataclass\n",
    "class DataCollatorSpeechSeq2SeqWithPadding:\n",
    "    processor: Any\n",
    "    padding: Union[bool, str] = True\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
    "        # Split inputs and labels since they have to be of different lengths and need different padding methods.\n",
    "        # \"input_features\" for Whisper-based models (vs. \"input_values\" for wav2vec...)\n",
    "        input_features = [{\"input_features\": feature[\"input_features\"]} for feature in features]\n",
    "        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
    "\n",
    "        batch = self.processor.feature_extractor.pad(input_features, \n",
    "                                                     return_tensors=\"pt\",\n",
    "                                                     return_attention_mask=True)\n",
    "        \n",
    "        labels_batch = self.processor.tokenizer.pad(label_features, return_tensors=\"pt\")\n",
    "\n",
    "        # Replace padding with -100 to ignore loss correctly\n",
    "        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
    "\n",
    "        batch[\"labels\"] = labels\n",
    "        return batch\n",
    "\n",
    "data_collator = DataCollatorSpeechSeq2SeqWithPadding(processor=processor, padding=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54a24f62",
   "metadata": {},
   "source": [
    "Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "179912e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model openai/whisper-medium.en loaded on cuda\n"
     ]
    }
   ],
   "source": [
    "from transformers import WhisperForConditionalGeneration\n",
    "\n",
    "model = WhisperForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "# This is necessary for the model to work correctly with the Trainer\n",
    "#model.config.forced_decoder_ids = None\n",
    "#model.config.suppress_tokens = []\n",
    "\n",
    "# send to the appropriate device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model.to(device)\n",
    "\n",
    "print(f'Model {model_name} loaded on {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "176c4d4a",
   "metadata": {},
   "source": [
    "If continuing to train..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "db9ed840",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m finetuned_model_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mwhisper-ft\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 2\u001b[0m model\u001b[38;5;241m.\u001b[39mload_state_dict(torch\u001b[38;5;241m.\u001b[39mload(finetuned_model_path, map_location\u001b[38;5;241m=\u001b[39mdevice))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "finetuned_model_path = \".\\\\whisper-ft\"\n",
    "model.load_state_dict(torch.load(finetuned_model_path, map_location=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "551df2f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Trainable parameters after freezing:\n",
      "  - model.decoder.embed_tokens.weight (Trainable, shape: torch.Size([51864, 1024]))\n",
      "\n",
      "Total trainable parameters: 53108736\n",
      "Total frozen parameters: 710748160\n",
      "Total parameters: 763856896\n",
      "Ratio of trained params to total params: 0.0695\n"
     ]
    }
   ],
   "source": [
    "# Freeze all parameters\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Except those in the last layer\n",
    "for param in model.proj_out.parameters():\n",
    "        param.requires_grad = True\n",
    "\n",
    "# Verify which layers are trainable\n",
    "print(\"\\nTrainable parameters after freezing:\")\n",
    "trainable_params = 0\n",
    "frozen_params = 0\n",
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        trainable_params += param.numel()\n",
    "        print(f\"  - {name} (Trainable, shape: {param.shape})\")\n",
    "    else:\n",
    "        frozen_params += param.numel()\n",
    "        # print(f\"  - {name} (Frozen)\") # Uncomment to see all frozen params\n",
    "\n",
    "total_params = trainable_params + frozen_params\n",
    "print(f\"\\nTotal trainable parameters: {trainable_params}\")\n",
    "print(f\"Total frozen parameters: {frozen_params}\")\n",
    "print(f\"Total parameters: {total_params}\")\n",
    "print(f\"Ratio of trained params to total params: {trainable_params / total_params:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27752e5c",
   "metadata": {},
   "source": [
    "Downsample if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c240d898",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5720e9e4f3db42dda5ca0e2ab1038076",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset from disk:   0%|          | 0/46 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Full Prepared Dataset ---\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['input_features', 'labels'],\n",
      "        num_rows: 23724\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['input_features', 'labels'],\n",
      "        num_rows: 5932\n",
      "    })\n",
      "})\n",
      "\n",
      "--- Sampled (10.0%) Dataset ---\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['input_features', 'labels'],\n",
      "        num_rows: 2372\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['input_features', 'labels'],\n",
      "        num_rows: 593\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_from_disk\n",
    "\n",
    "sample_percentage = 0.1\n",
    "\n",
    "# Load full prepared dataset\n",
    "prepared_dataset_path = 'dataset_whisper'\n",
    "prepared_datasets = load_from_disk(prepared_dataset_path)\n",
    "print(\"--- Full Prepared Dataset ---\")\n",
    "print(prepared_datasets)\n",
    "\n",
    "# Sample 1% from the training set\n",
    "train_split = prepared_datasets[\"train\"]\n",
    "sampled_train_split = train_split.train_test_split(train_size=sample_percentage, shuffle=True, seed=555)['train'] # We only want the 'train' part of this new split\n",
    "\n",
    "test_split = prepared_datasets[\"test\"]\n",
    "sampled_test_split = test_split.train_test_split(train_size=sample_percentage, shuffle=True, seed=555)['train'] \n",
    "\n",
    "# Overwrite the original splits with the sampled splits\n",
    "prepared_datasets['train'] = sampled_train_split\n",
    "prepared_datasets['test'] = sampled_test_split\n",
    "\n",
    "print(f\"\\n--- Sampled ({sample_percentage*100}%) Dataset ---\")\n",
    "print(prepared_datasets)\n",
    "\n",
    "# Now, use this smaller `prepared_datasets` object for the rest of your script\n",
    "# (creating DataLoaders, etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "443b335a",
   "metadata": {},
   "source": [
    "Training parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d562d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import AdamW\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "\n",
    "def remove_punctuation(s):\n",
    "    s = re.sub(r'[^a-zA-Z0-9\\s]', '', s)\n",
    "    return s.lower()\n",
    "\n",
    "# Training parameters\n",
    "learning_rate = .0002\n",
    "train_batch_size = 8\n",
    "eval_batch_size = 16\n",
    "\n",
    "# Defined train and test DLs\n",
    "train_dataloader = DataLoader(prepared_datasets[\"train\"], shuffle=True, collate_fn=data_collator, batch_size=train_batch_size)\n",
    "eval_dataloader = DataLoader(prepared_datasets[\"test\"], collate_fn=data_collator, batch_size=eval_batch_size)\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
    "scaler = torch.amp.GradScaler('cuda')\n",
    "\n",
    "wer_metric = evaluate.load(\"wer\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa183dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import get_scheduler\n",
    "\n",
    "num_train_epochs = 20\n",
    "num_warmup_steps = 100\n",
    "total_steps = len(train_dataloader) * num_train_epochs\n",
    "\n",
    "lr_scheduler = get_scheduler(name=\"linear\",\n",
    "                             optimizer=optimizer,\n",
    "                             num_warmup_steps=num_warmup_steps,\n",
    "                             num_training_steps=total_steps)\n",
    "\n",
    "# Set initial WER max to inf\n",
    "best_wer = float('inf')\n",
    "output_dir = \".\\\\whisper-ft\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbaf9c10",
   "metadata": {},
   "source": [
    "Main training cycle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "447f352c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1/20:   0%|          | 0/297 [00:00<?, ?it/s]Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.43.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n",
      "Training Epoch 1/20:  19%|█▉        | 57/297 [01:14<05:05,  1.27s/it]"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_train_epochs):\n",
    "    # --- TRAINING ---\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    \n",
    "    # Use tqdm for a progress bar\n",
    "    for step, batch in enumerate(tqdm(train_dataloader, desc=f\"Training Epoch {epoch + 1}/{num_train_epochs}\")):\n",
    "        # Move batch to device\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "\n",
    "        # Backward pass\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    avg_train_loss = train_loss / len(train_dataloader)\n",
    "    print(f\"Epoch {epoch + 1} | Average Training Loss: {avg_train_loss:.4f}\")\n",
    "\n",
    "    # --- EVALUATION ---\n",
    "    model.eval()\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "\n",
    "    # Use torch.no_grad() to save memory and computations\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(eval_dataloader, desc=\"Evaluating\"):\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            \n",
    "            # Generate predictions. This is different from the training forward pass.\n",
    "            generated_ids = model.generate(input_features=batch[\"input_features\"], \n",
    "                                           attention_mask=batch[\"attention_mask\"], \n",
    "                                           max_length=225)\n",
    "            \n",
    "            # Decode predictions\n",
    "            predictions = processor.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "            \n",
    "            # Decode labels, replacing -100 with pad token\n",
    "            labels = batch[\"labels\"].clone()\n",
    "            labels[labels == -100] = processor.tokenizer.pad_token_id\n",
    "            labels_str = processor.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "            # Remove punctuation and capital letters from transcription\n",
    "            predictions = [remove_punctuation(p) for p in predictions]\n",
    "\n",
    "            all_predictions.extend(predictions)\n",
    "            all_labels.extend(labels_str)\n",
    "\n",
    "    # Compute WER\n",
    "    wer = wer_metric.compute(predictions=all_predictions, references=all_labels)\n",
    "    \n",
    "    print(f\"WER: {wer:.4f}\")\n",
    "\n",
    "    # Save the model if it has the best WER so far\n",
    "    if wer < best_wer:\n",
    "        best_wer = wer\n",
    "        print(f\"New best WER: {best_wer}. Saving model...\")\n",
    "        model.save_pretrained(output_dir)\n",
    "        processor.save_pretrained(output_dir)\n",
    "        print(f\"Model saved to {output_dir}\")\n",
    "\n",
    "print(\"\\n--- Training Complete ---\")\n",
    "print(f\"Best WER achieved: {best_wer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a804e025",
   "metadata": {},
   "source": [
    "--------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e1ddab3",
   "metadata": {},
   "source": [
    "Testing a trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a622cbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading audio from: reg.wav...\n",
      "Generating transcription...\n",
      "\n",
      "Transcription:\n",
      "  It was a clear black night, a clear white moon Warren G was on the streets trying to consume some skirts for the E so I could get some phones rolling in my ride, chilling all alone Just hit the east side of the LBC on a mission trying to find Mr. Warren G. Seen a call full of girls, ain't no need to tweak all you skirts know what's up with 213 So I hooked Celeste\n"
     ]
    }
   ],
   "source": [
    "import torchaudio\n",
    "\n",
    "def test_transcribe(audio_path):\n",
    "    # Put in evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    # Load audio file\n",
    "    print(f\"Loading audio from: {audio_path}...\")\n",
    "    waveform, sample_rate = torchaudio.load(audio_path)\n",
    "\n",
    "    # Resample if necessary (Whisper expects 16kHz)\n",
    "    if sample_rate != 16000:\n",
    "        print(f\"Resampling audio from {sample_rate}Hz to 16kHz...\")\n",
    "        resampler = torchaudio.transforms.Resample(orig_freq=sample_rate, new_freq=16000)\n",
    "        waveform = resampler(waveform)\n",
    "        sample_rate = 16000 # Update sample rate after resampling\n",
    "\n",
    "    # Ensure mono audio (Whisper expects single channel)\n",
    "    if waveform.shape[0] > 1:\n",
    "        print(\"Converting stereo audio to mono...\")\n",
    "        waveform = waveform.mean(dim=0, keepdim=True) # Average channels to mono\n",
    "\n",
    "    # Convert to numpy array (required by feature_extractor for raw audio)\n",
    "    audio_array = waveform.squeeze().numpy()\n",
    "\n",
    "    # Extract features (Mel spectrogram)\n",
    "    processed_audio = processor.feature_extractor(audio_array, \n",
    "                                                sampling_rate=sample_rate, \n",
    "                                                return_tensors=\"pt\",\n",
    "                                                return_attention_mask=True)\n",
    "\n",
    "    input_features = processed_audio.input_features.to(device)\n",
    "    attention_mask = processed_audio.attention_mask.to(device)\n",
    "\n",
    "    print(\"Generating transcription...\")\n",
    "    with torch.no_grad():\n",
    "        generated_ids = model.generate(input_features=input_features, \n",
    "                                       attention_mask=attention_mask,\n",
    "                                       max_new_tokens=256,\n",
    "                                       temperature=0.0,\n",
    "                                       #no_speech_threshold=.3 # Error when using this ?\n",
    "                                       )\n",
    "        \n",
    "    # Create the transcription\n",
    "    transcription = processor.tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "    return transcription\n",
    "\n",
    "\n",
    "# Load and preprocess the audio file\n",
    "audio_path = 'reg.wav'\n",
    "print(\"\\nTranscription:\\n\", test_transcribe(audio_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c8e3bdd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
