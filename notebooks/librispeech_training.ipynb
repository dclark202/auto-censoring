{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Loading dataset and instantiating"
      ],
      "metadata": {
        "id": "VyUyGTT0HACW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "zH3awnxQfJ4D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q evaluate"
      ],
      "metadata": {
        "id": "mts7CylSM66E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q jiwer"
      ],
      "metadata": {
        "id": "wnHtStGdM-FC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from datasets import load_from_disk\n"
      ],
      "metadata": {
        "id": "aZPw5_tXHPs8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_dataset = load_from_disk(\"/content/drive/MyDrive/audio-to-text/DALI/dataset_processed\")"
      ],
      "metadata": {
        "id": "4cbhsY9yHB1z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_dataset[0].keys()"
      ],
      "metadata": {
        "id": "T3RlWAqJVbeY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Any, Dict, List, Union\n",
        "import torch\n",
        "\n",
        "class DataCollatorSpeechSeq2SeqWithPadding:\n",
        "    def __init__(self, processor: Any, label_pad_token_id: int = -100):\n",
        "        self.processor = processor\n",
        "        self.label_pad_token_id = label_pad_token_id\n",
        "\n",
        "    def __call__(\n",
        "        self, features: List[Dict[str, Union[List[int], torch.Tensor]]]\n",
        "    ) -> Dict[str, torch.Tensor]:\n",
        "        # Extract and pad audio features\n",
        "        input_features = [\n",
        "            {\"input_features\": feature[\"input_features\"]} for feature in features\n",
        "        ]\n",
        "        batch = self.processor.feature_extractor.pad(\n",
        "            input_features,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "\n",
        "        # Extract and pad tokenized labels\n",
        "        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
        "        labels_batch = self.processor.tokenizer.pad(\n",
        "            label_features,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "\n",
        "        # Replace padding token with -100 for loss ignoring\n",
        "        labels = labels_batch[\"input_ids\"].masked_fill(\n",
        "            labels_batch.attention_mask.ne(1), self.label_pad_token_id\n",
        "        )\n",
        "\n",
        "        # Remove BOS if it's automatically added (to avoid duplication)\n",
        "        if (labels[:, 0] == self.processor.tokenizer.bos_token_id).all():\n",
        "            labels = labels[:, 1:]\n",
        "\n",
        "        batch[\"labels\"] = labels\n",
        "\n",
        "        return batch\n"
      ],
      "metadata": {
        "id": "W8LZKyhSnplp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install -U transformers\n"
      ],
      "metadata": {
        "id": "ed3xU4g55LQj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Speech2TextProcessor, Speech2TextForConditionalGeneration\n",
        "\n",
        "processor = Speech2TextProcessor.from_pretrained(\"facebook/s2t-small-librispeech-asr\")\n"
      ],
      "metadata": {
        "id": "0Ije2XAMpIwd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_collator = DataCollatorSpeechSeq2SeqWithPadding(processor)\n"
      ],
      "metadata": {
        "id": "1KPYvTFg5LS1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import KFold\n",
        "\n",
        "k = 5\n",
        "kf = KFold(n_splits=k, shuffle=True, random_state=42)"
      ],
      "metadata": {
        "id": "qlm--le3p4pm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training"
      ],
      "metadata": {
        "id": "rw15vT9XRpHK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import evaluate\n",
        "\n",
        "wer_metric = evaluate.load(\"wer\")\n"
      ],
      "metadata": {
        "id": "QdLCdSfKs47K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.utils as nn_utils\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.optim import AdamW\n",
        "from transformers import get_scheduler\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "import shutil\n",
        "import time\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "best_wer = 1.0\n",
        "best_model_dir = None\n",
        "\n",
        "for fold, (train_idx, val_idx) in enumerate(kf.split(final_dataset)):\n",
        "\n",
        "    print(f\"\\n=== Fold {fold+1}/{kf.n_splits} ===\")\n",
        "\n",
        "    train_dataset = final_dataset.select(train_idx.tolist())\n",
        "    val_dataset = final_dataset.select(val_idx.tolist())\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True, collate_fn=data_collator)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=4, shuffle=False, collate_fn=data_collator)\n",
        "\n",
        "    model = Speech2TextForConditionalGeneration.from_pretrained(\"facebook/s2t-small-librispeech-asr\")\n",
        "\n",
        "    for name, param in model.named_parameters():\n",
        "      if name.startswith(\"encoder.conv\"):\n",
        "        param.requires_grad = False\n",
        "      else:\n",
        "        param.requires_grad = True\n",
        "\n",
        "\n",
        "    model.to(device)\n",
        "\n",
        "    optimizer = AdamW(model.parameters(), lr=5e-5)\n",
        "    num_epochs = 10\n",
        "    num_training_steps = num_epochs * len(train_loader)\n",
        "    lr_scheduler = get_scheduler(\n",
        "        \"linear\",\n",
        "        optimizer=optimizer,\n",
        "        num_warmup_steps=500,\n",
        "        num_training_steps=num_training_steps,\n",
        "    )\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        total_loss = 0.0\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
        "        for step, batch in enumerate(tqdm(train_loader)):\n",
        "            batch = {k: v.to(device) for k, v in batch.items()}\n",
        "            try:\n",
        "                outputs = model(\n",
        "                    input_features=batch[\"input_features\"],\n",
        "                    attention_mask=batch[\"attention_mask\"],\n",
        "                    labels=batch[\"labels\"]\n",
        "                )\n",
        "                loss = outputs.loss\n",
        "\n",
        "                if not torch.isfinite(loss):\n",
        "                    print(f\"Warning: Non-finite loss at step {step}, skipping batch\")\n",
        "                    optimizer.zero_grad()\n",
        "                    continue\n",
        "\n",
        "                loss.backward()\n",
        "                nn_utils.clip_grad_norm_(model.parameters(), max_norm=0.5)\n",
        "                optimizer.step()\n",
        "                lr_scheduler.step()\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                total_loss += loss.item()\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Exception during forward pass at step {step}: {e}\")\n",
        "                raise\n",
        "\n",
        "        avg_train_loss = total_loss / len(train_loader)\n",
        "        print(f\"Avg training loss: {avg_train_loss:.4f}\")\n",
        "\n",
        "        # Validation\n",
        "        model.eval()\n",
        "        all_preds = []\n",
        "        all_refs = []\n",
        "\n",
        "        for batch in tqdm(val_loader):\n",
        "            batch = {k: v.to(device) for k, v in batch.items()}\n",
        "            with torch.no_grad():\n",
        "                generated_ids = model.generate(\n",
        "                    input_features=batch[\"input_features\"],\n",
        "                    attention_mask=batch[\"attention_mask\"],\n",
        "                    max_length=128,\n",
        "                )\n",
        "\n",
        "            preds = processor.batch_decode(generated_ids, skip_special_tokens=True)\n",
        "            labels = batch[\"labels\"].cpu().numpy()\n",
        "            # Replace -100 with pad_token_id before decoding\n",
        "            labels = np.where(labels == -100, processor.tokenizer.pad_token_id, labels)\n",
        "            refs = processor.batch_decode(labels, skip_special_tokens=True)\n",
        "\n",
        "            all_preds.extend(preds)\n",
        "            all_refs.extend(refs)\n",
        "\n",
        "        wer = wer_metric.compute(predictions=all_preds, references=all_refs)\n",
        "        print(f\"Validation WER: {wer:.4f}\")\n",
        "\n",
        "        if wer < best_wer:\n",
        "            if best_model_dir and os.path.exists(best_model_dir):\n",
        "                shutil.rmtree(best_model_dir)\n",
        "\n",
        "            best_wer = wer\n",
        "            timestamp = int(time.time())\n",
        "            best_model_dir = f\"./checkpoint-fold{fold}-epoch{epoch}-wer{wer:.4f}-{timestamp}\"\n",
        "            os.makedirs(best_model_dir, exist_ok=True)\n",
        "            model.save_pretrained(best_model_dir)\n",
        "            processor.save_pretrained(best_model_dir)\n",
        "            print(f\"Saved best model to {best_model_dir}\")\n",
        "\n",
        "print(f\"\\nTraining finished. Best WER: {best_wer:.4f}\")\n"
      ],
      "metadata": {
        "id": "lzlGIY_0ZdJy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def transcribe_wav(wav_path, model_path):\n",
        "    processor = Speech2TextProcessor.from_pretrained(model_path)\n",
        "    model = Speech2TextForConditionalGeneration.from_pretrained(model_path)\n",
        "    model.to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    waveform, sr = torchaudio.load(wav_path)\n",
        "    waveform = torchaudio.transforms.Resample(orig_freq=sr, new_freq=16000)(waveform).squeeze().numpy()\n",
        "\n",
        "    inputs = processor(waveform, sampling_rate=16000, return_tensors=\"pt\")\n",
        "    inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
        "\n",
        "    generated_ids = model.generate(input_features=inputs[\"input_features\"], max_length=128)\n",
        "    return processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n"
      ],
      "metadata": {
        "id": "znOkgMf8wtdx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}