{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "34e893d4",
   "metadata": {},
   "source": [
    "Ideas ----\n",
    "- To check for which songs are edited in the youtube downloads, download youtube transcription ctrl+f for curse words or \"[ ___ ]\"\n",
    "- Order training set by size to minimize padding tokens?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1b1438a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file</th>\n",
       "      <th>transcript</th>\n",
       "      <th>file-wav</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ff3c695eb32e4197924e7786e8a7812f-7.mp3</td>\n",
       "      <td>all that i want is stillness of heart so i can...</td>\n",
       "      <td>ff3c695eb32e4197924e7786e8a7812f-7.wav</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>b3639d60a49e45578a201d910f36c44c-5.mp3</td>\n",
       "      <td>the first time to really feel alive the first ...</td>\n",
       "      <td>b3639d60a49e45578a201d910f36c44c-5.wav</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>81275468fa124185a222bf037764e925-1.mp3</td>\n",
       "      <td>a new day is rising the queen is laughing stil...</td>\n",
       "      <td>81275468fa124185a222bf037764e925-1.wav</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0ea248a9588641749edeae319b6ed3ac-0.mp3</td>\n",
       "      <td>getting edgy all the time someone around me ju...</td>\n",
       "      <td>0ea248a9588641749edeae319b6ed3ac-0.wav</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>75ad1213a743497185a0b13cb11d4a37-4.mp3</td>\n",
       "      <td>therell never be a moment ill regret ive loved...</td>\n",
       "      <td>75ad1213a743497185a0b13cb11d4a37-4.wav</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     file  \\\n",
       "0  ff3c695eb32e4197924e7786e8a7812f-7.mp3   \n",
       "1  b3639d60a49e45578a201d910f36c44c-5.mp3   \n",
       "2  81275468fa124185a222bf037764e925-1.mp3   \n",
       "3  0ea248a9588641749edeae319b6ed3ac-0.mp3   \n",
       "4  75ad1213a743497185a0b13cb11d4a37-4.mp3   \n",
       "\n",
       "                                          transcript  \\\n",
       "0  all that i want is stillness of heart so i can...   \n",
       "1  the first time to really feel alive the first ...   \n",
       "2  a new day is rising the queen is laughing stil...   \n",
       "3  getting edgy all the time someone around me ju...   \n",
       "4  therell never be a moment ill regret ive loved...   \n",
       "\n",
       "                                 file-wav  \n",
       "0  ff3c695eb32e4197924e7786e8a7812f-7.wav  \n",
       "1  b3639d60a49e45578a201d910f36c44c-5.wav  \n",
       "2  81275468fa124185a222bf037764e925-1.wav  \n",
       "3  0ea248a9588641749edeae319b6ed3ac-0.wav  \n",
       "4  75ad1213a743497185a0b13cb11d4a37-4.wav  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "# Set directory for audio chunks and load csv with lyrics\n",
    "chunks_dir = \"C:\\\\Users\\\\dacla\\\\Documents\\\\DALI-chunks\"\n",
    "df_chunks = pd.read_csv(\"lyrics-chunks-train.csv\")\n",
    "\n",
    "df_chunks.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97967160",
   "metadata": {},
   "source": [
    "Create tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a3c4ac70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['C:\\\\Users\\\\dacla\\\\Documents\\\\auto-censoring-local\\\\tokenizers\\\\vocab.json',\n",
       " 'C:\\\\Users\\\\dacla\\\\Documents\\\\auto-censoring-local\\\\tokenizers\\\\merges.txt']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "\n",
    "tokenizer = Tokenizer(BPE(unk_token=\"[UNK]\"))\n",
    "\n",
    "# Split by whitespace\n",
    "tokenizer.pre_tokenizer = Whitespace()\n",
    "\n",
    "# Byte-pair encoding\n",
    "trainer = BpeTrainer(vocab_size=1000, min_frequency=5, special_tokens=[\"[PAD]\", \"[UNK]\", \"|\"])\n",
    "\n",
    "# Text body from the DALI lyrics database\n",
    "file_path = \"C:\\\\Users\\\\dacla\\\\Documents\\\\auto-censoring-local\\\\corpus.txt\"\n",
    "\n",
    "# Train the tokenizer\n",
    "tokenizer.train([file_path], trainer)\n",
    "\n",
    "# And save output\n",
    "token_dir = \"C:\\\\Users\\\\dacla\\\\Documents\\\\auto-censoring-local\\\\tokenizers\"\n",
    "tokenizer.save(f\"{token_dir}\\\\tokenizer.json\")\n",
    "tokenizer.model.save(token_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dc12ee5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: ['be', 'an', 's', 'and', 'le', 'gu', 'me', 's']\n",
      "IDs: [59, 42, 31, 53, 64, 350, 47, 31]\n",
      "\n",
      "ID for [PAD]: 0\n",
      "ID for [UNK]: 1\n",
      "ID for '|' (space): 2\n"
     ]
    }
   ],
   "source": [
    "# Test the tokenizer\n",
    "encoded = tokenizer.encode(\"beans and legumes\")\n",
    "print(f\"Tokens: {encoded.tokens}\")\n",
    "print(f\"IDs: {encoded.ids}\")\n",
    "print()\n",
    "\n",
    "\n",
    "print(f\"ID for [PAD]: {tokenizer.token_to_id('[PAD]')}\")\n",
    "print(f\"ID for [UNK]: {tokenizer.token_to_id('[UNK]')}\")\n",
    "print(f\"ID for '|' (space): {tokenizer.token_to_id('|')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "17bf2e2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('.\\\\tokenizers\\\\my_wav2vec2_bpe_tokenizer\\\\tokenizer_config.json',\n",
       " '.\\\\tokenizers\\\\my_wav2vec2_bpe_tokenizer\\\\special_tokens_map.json',\n",
       " '.\\\\tokenizers\\\\my_wav2vec2_bpe_tokenizer\\\\vocab.json',\n",
       " '.\\\\tokenizers\\\\my_wav2vec2_bpe_tokenizer\\\\added_tokens.json')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import Wav2Vec2CTCTokenizer\n",
    "\n",
    "# Path to the files we just saved\n",
    "vocab_file = \".\\\\tokenizers\\\\vocab.json\"\n",
    "merges_file = \".\\\\tokenizers\\\\merges.txt\"\n",
    "\n",
    "# Load the trained BPE files into the wav2vec2-specific tokenizer class\n",
    "custom_tokenizer = Wav2Vec2CTCTokenizer(\n",
    "    vocab_file=vocab_file,\n",
    "    merges_file=merges_file,\n",
    "    unk_token=\"[UNK]\",\n",
    "    pad_token=\"[PAD]\",\n",
    "    word_delimiter_token=\"|\", # Crucial for wav2vec2\n",
    ")\n",
    "\n",
    "custom_tokenizer.save_pretrained(\".\\\\tokenizers\\\\my_wav2vec2_bpe_tokenizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "09bcf757",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: ['bea', 'n', 's', 'and', 'le', 'gu', 'me', 's']\n",
      "Encoded IDs: [552, 26, 31, 53, 64, 350, 47, 31]\n"
     ]
    }
   ],
   "source": [
    "test_sentence = 'beans and legumes'\n",
    "\n",
    "tokens = custom_tokenizer.tokenize(test_sentence)\n",
    "print(\"Tokens:\", tokens)\n",
    "\n",
    "encoded = custom_tokenizer(test_sentence).input_ids\n",
    "print(\"Encoded IDs:\", encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e595073",
   "metadata": {},
   "source": [
    "Create Processor using custom tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "826c9b38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processor created and saved.\n"
     ]
    }
   ],
   "source": [
    "from transformers import Wav2Vec2FeatureExtractor, Wav2Vec2Processor\n",
    "\n",
    "# 1. Load your custom tokenizer\n",
    "tokenizer = Wav2Vec2CTCTokenizer.from_pretrained(\".\\\\tokenizers\\\\my_wav2vec2_bpe_tokenizer\")\n",
    "\n",
    "# 2. Create a standard feature extractor\n",
    "feature_extractor = Wav2Vec2FeatureExtractor(\n",
    "    feature_size=1,\n",
    "    sampling_rate=16000,\n",
    "    padding_value=0.0,\n",
    "    do_normalize=True,\n",
    "    return_attention_mask=False\n",
    ")\n",
    "\n",
    "# 3. Bundle them into a processor\n",
    "processor = Wav2Vec2Processor(feature_extractor=feature_extractor, tokenizer=tokenizer)\n",
    "\n",
    "# Save the processor for easy loading later\n",
    "processor.save_pretrained(\"my_wav2vec2_processor\")\n",
    "print(\"Processor created and saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cd6dfb7",
   "metadata": {},
   "source": [
    "Prepare dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9572f1ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>transcript</th>\n",
       "      <th>file-wav</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>life is a moment in space when the dream is go...</td>\n",
       "      <td>001940b614eb43f4a0c826d49a67d66d-0.wav</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>i kiss the morning goodbye butdown inside you ...</td>\n",
       "      <td>001940b614eb43f4a0c826d49a67d66d-1.wav</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>the road is narrow and long when eyes meet eye...</td>\n",
       "      <td>001940b614eb43f4a0c826d49a67d66d-2.wav</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>i turn away from the wall i stumble and fall b...</td>\n",
       "      <td>001940b614eb43f4a0c826d49a67d66d-3.wav</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>i am a woman in love and id do anything to get...</td>\n",
       "      <td>001940b614eb43f4a0c826d49a67d66d-4.wav</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          transcript  \\\n",
       "0  life is a moment in space when the dream is go...   \n",
       "1  i kiss the morning goodbye butdown inside you ...   \n",
       "2  the road is narrow and long when eyes meet eye...   \n",
       "3  i turn away from the wall i stumble and fall b...   \n",
       "4  i am a woman in love and id do anything to get...   \n",
       "\n",
       "                                 file-wav  \n",
       "0  001940b614eb43f4a0c826d49a67d66d-0.wav  \n",
       "1  001940b614eb43f4a0c826d49a67d66d-1.wav  \n",
       "2  001940b614eb43f4a0c826d49a67d66d-2.wav  \n",
       "3  001940b614eb43f4a0c826d49a67d66d-3.wav  \n",
       "4  001940b614eb43f4a0c826d49a67d66d-4.wav  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# metadata-wav has *only* those files which successfully converted to wav 16kHz mono\n",
    "data = pd.read_csv('metadata-wav.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "544e4c32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['transcript', 'file-wav'],\n",
      "    num_rows: 29656\n",
      "})\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['transcript', 'file-wav'],\n",
      "        num_rows: 23724\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['transcript', 'file-wav'],\n",
      "        num_rows: 5932\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, Audio, DatasetDict\n",
    "\n",
    "dataset_path = \"C:\\\\Users\\\\dacla\\\\Documents\\\\DALI-chunks-wav\"\n",
    "\n",
    "full_dataset = load_dataset(\"csv\", data_files=\"metadata-wav.csv\", split='train')\n",
    "print(\"Full dataset\", full_dataset)\n",
    "\n",
    "# Make a train/test split at this point !\n",
    "split_dataset = full_dataset.train_test_split(test_size=0.2, shuffle=True, seed=555)\n",
    "print(\"\\nSplit dataset\", split_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "315f154a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af0c1f37394449f7bb0f1e6e280543e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/23724 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "562e96e8a1774082a701a3b6352229e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5932 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['input_values', 'labels'],\n",
      "        num_rows: 23724\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['input_values', 'labels'],\n",
      "        num_rows: 5932\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "import librosa # for faster comuputation?\n",
    "\n",
    "def prepare_dataset(batch):\n",
    "        audio_paths = [f\"{dataset_path}\\\\{fname}\" for fname in batch['file-wav']]\n",
    "        audio_arrays = [librosa.load(path, sr=16000)[0] for path in audio_paths]\n",
    "\n",
    "        # Needed for custom processor\n",
    "        model_inputs = processor.feature_extractor(audio_arrays, sampling_rate=16000, padding=\"longest\", return_tensors=\"pt\")\n",
    "        batch[\"input_values\"] = model_inputs.input_values\n",
    "\n",
    "        labels = processor.tokenizer(batch[\"transcript\"], padding=\"longest\").input_ids\n",
    "        \n",
    "        # CTC loss ignores labels with value -100\n",
    "        batch[\"labels\"] = [[label if label != processor.tokenizer.pad_token_id else -100 for label in T] for T in labels]\n",
    "        return batch\n",
    "\n",
    "prepared_dataset = split_dataset.map(prepare_dataset, \n",
    "                                     batched=True,\n",
    "                                     batch_size=8, # 8 might be too big...check VRAM usage\n",
    "                                     remove_columns=split_dataset['train'].column_names)\n",
    "\n",
    "print(prepared_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb728bd6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa03e97b049244c58bf0b2274307e2c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/120 shards):   0%|          | 0/23724 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3830ddaf279d4391b216b9917dbfd24a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/30 shards):   0%|          | 0/5932 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Save dataset to disc\n",
    "prepared_dataset.save_to_disk('dataset_prepared')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e979cea",
   "metadata": {},
   "source": [
    "Prepare for training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "809be928",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, Dict, List, Union\n",
    "import torch\n",
    "\n",
    "# --- Data Collator ---\n",
    "@dataclass\n",
    "class DataCollatorCTCWithPadding:\n",
    "    processor: Wav2Vec2Processor\n",
    "    padding: Union[bool, str] = True\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
    "        # Split inputs and labels since they have to be of different lengths\n",
    "        # and need different padding methods.\n",
    "        input_features = [{\"input_values\": feature[\"input_values\"]} for feature in features]\n",
    "        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
    "\n",
    "        batch = self.processor.pad(\n",
    "            input_features,\n",
    "            padding=self.padding,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        \n",
    "        labels_batch = self.processor.tokenizer.pad(\n",
    "            label_features,\n",
    "            padding=self.padding,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "\n",
    "        # Replace padding with -100 to ignore loss correctly\n",
    "        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
    "\n",
    "        batch[\"labels\"] = labels\n",
    "        return batch\n",
    "\n",
    "data_collator = DataCollatorCTCWithPadding(processor=processor, padding=\"longest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "61743283",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_dataset_path = \"C:\\\\Users\\\\dacla\\\\Documents\\\\auto-censoring-local\\\\dataset_prepared\"\n",
    "model_checkpoint = \"facebook/wav2vec2-base-960h\"\n",
    "\n",
    "wer_metric = evaluate.load(\"wer\")\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    # Decode predicted IDs to text\n",
    "    pred_logits = pred.predictions\n",
    "    pred_ids = torch.argmax(torch.from_numpy(pred_logits), dim=-1)\n",
    "    \n",
    "    # Decode true labels\n",
    "    pred.label_ids[pred.label_ids == -100] = processor.tokenizer.pad_token_id\n",
    "    pred_str = processor.batch_decode(pred_ids)\n",
    "    label_str = processor.batch_decode(pred.label_ids, group_tokens=False)\n",
    "\n",
    "    wer = wer_metric.compute(predictions=pred_str, references=label_str)\n",
    "    return {\"wer\": wer}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8817e3fc",
   "metadata": {},
   "source": [
    "Create model and rewrite classification layer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be2a84ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized: ['wav2vec2.masked_spec_embed']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Old LM Head: Linear(in_features=768, out_features=32, bias=True)\n",
      "Old Vocab Size (from config): 32\n",
      "--------------------\n",
      "New LM Head: Linear(in_features=768, out_features=1000, bias=True)\n",
      "New Vocab Size (from config): 1000\n"
     ]
    }
   ],
   "source": [
    "from transformers import Wav2Vec2ForCTC\n",
    "\n",
    "new_vocab_size = processor.tokenizer.vocab_size\n",
    "\n",
    "model_name = \"facebook/wav2vec2-base-960h\"\n",
    "model = Wav2Vec2ForCTC.from_pretrained(model_name, ctc_loss_reduction='mean', pad_token_id=processor.tokenizer.pad_token_id)\n",
    "\n",
    "print(f\"Old LM Head: {model.lm_head}\")\n",
    "print(f\"Old Vocab Size (from config): {model.config.vocab_size}\")\n",
    "\n",
    "### Manually replace the LM head\n",
    "# Get the model's hidden size\n",
    "hidden_size = model.config.hidden_size\n",
    "\n",
    "# Create a new linear layer with the correct dimensions\n",
    "new_lm_head = torch.nn.Linear(hidden_size, new_vocab_size)\n",
    "\n",
    "# Replace the old lm_head with the new one\n",
    "model.lm_head = new_lm_head\n",
    "\n",
    "# Update the model's config to reflect the new vocab size\n",
    "model.config.vocab_size = new_vocab_size\n",
    "model.config.pad_token_id = processor.tokenizer.pad_token_id\n",
    "\n",
    "print(\"-\" * 20)\n",
    "print(f\"New LM Head: {model.lm_head}\")\n",
    "print(f\"New Vocab Size (from config): {model.config.vocab_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "83a65f1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model facebook/wav2vec2-base-960h loaded on cuda\n"
     ]
    }
   ],
   "source": [
    "# send to the appropriate device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model.to(device)\n",
    "\n",
    "# It's a good practice to freeze the feature extractor part of the model\n",
    "model.freeze_feature_encoder()\n",
    "\n",
    "print(f'Model {model_name} loaded on {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8577f157",
   "metadata": {},
   "source": [
    "Downsample the dataset for testing the training loops\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6cbf2fdc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3dd9158a891443ef826a2cad57f2fdf0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset from disk:   0%|          | 0/120 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3dfa99a0a1024f2ba8ea6919aed3583b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset from disk:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Full Prepared Dataset ---\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['input_values', 'labels'],\n",
      "        num_rows: 23724\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['input_values', 'labels'],\n",
      "        num_rows: 5932\n",
      "    })\n",
      "})\n",
      "\n",
      "--- Sampled (1.0%) Dataset ---\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['input_values', 'labels'],\n",
      "        num_rows: 237\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['input_values', 'labels'],\n",
      "        num_rows: 59\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_from_disk\n",
    "\n",
    "sample_percentage = 0.01 \n",
    "\n",
    "# Load full prepared dataset\n",
    "prepared_dataset_path = 'dataset_prepared'\n",
    "prepared_datasets = load_from_disk(prepared_dataset_path)\n",
    "print(\"--- Full Prepared Dataset ---\")\n",
    "print(prepared_datasets)\n",
    "\n",
    "# Sample 1% from the training set\n",
    "train_split = prepared_datasets[\"train\"]\n",
    "sampled_train_split = train_split.train_test_split(train_size=sample_percentage, shuffle=True, seed=555)['train'] # We only want the 'train' part of this new split\n",
    "\n",
    "test_split = prepared_datasets[\"test\"]\n",
    "sampled_test_split = test_split.train_test_split(train_size=sample_percentage, shuffle=True, seed=555)['train'] \n",
    "\n",
    "# Overwrite the original splits with the sampled splits\n",
    "prepared_datasets['train'] = sampled_train_split\n",
    "prepared_datasets['test'] = sampled_test_split\n",
    "\n",
    "print(f\"\\n--- Sampled ({sample_percentage*100}%) Dataset ---\")\n",
    "print(prepared_datasets)\n",
    "\n",
    "# Now, use this smaller `prepared_datasets` object for the rest of your script\n",
    "# (creating DataLoaders, etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8c512ac",
   "metadata": {},
   "source": [
    "Training parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "00711fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from transformers import get_scheduler\n",
    "from torch.optim import AdamW\n",
    "\n",
    "# Training parameters\n",
    "learning_rate = .001\n",
    "train_batch_size = 2\n",
    "eval_batch_size = 4\n",
    "\n",
    "# Defined train and test DLs\n",
    "train_dataloader = DataLoader(\n",
    "    prepared_datasets[\"train\"],\n",
    "    shuffle=True,\n",
    "    collate_fn=data_collator,\n",
    "    batch_size=train_batch_size\n",
    ")\n",
    "\n",
    "eval_dataloader = DataLoader(\n",
    "    prepared_datasets[\"test\"],\n",
    "    collate_fn=data_collator,\n",
    "    batch_size=eval_batch_size\n",
    ")\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
    "## LR scheduler..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40185aaa",
   "metadata": {},
   "source": [
    "Main training loop. Needs more automation for saving best models, etc. But I'm trying to just get it to do anything at the moment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0be85ee2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 11/119 [00:09<01:26,  1.25it/s]"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm \n",
    "\n",
    "num_epochs = 30 \n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # --- Training Phase ---\n",
    "    model.train()\n",
    "    for batch in tqdm(train_dataloader):\n",
    "        # Move batch to the correct device\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        \n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "    print(f\"Epoch {epoch+1}, Loss: {loss.item():.4f}\")\n",
    "\n",
    "    # --- Evaluation Phase ---\n",
    "    model.eval()\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "    \n",
    "    for batch in tqdm(eval_dataloader):\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model(**batch)\n",
    "        \n",
    "        logits = outputs.logits\n",
    "        predicted_ids = torch.argmax(logits, dim=-1)\n",
    "        \n",
    "        all_predictions.extend(predicted_ids.cpu().numpy())\n",
    "        all_labels.extend(batch[\"labels\"].cpu().numpy())\n",
    "        \n",
    "    # Compute WER metric\n",
    "    metric_result = compute_metrics(pred_ids=all_predictions, label_ids=all_labels)\n",
    "    print(f\"Epoch {epoch+1} WER: {metric_result['wer']:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fc2b680",
   "metadata": {},
   "source": [
    "-----------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61f3f7ea",
   "metadata": {},
   "source": [
    "OLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "79a0d07f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dacla\\AppData\\Local\\Temp\\ipykernel_38064\\2401222778.py:15: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler()\n"
     ]
    }
   ],
   "source": [
    "from transformers import get_scheduler\n",
    "from torch.optim import AdamW\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "num_training_steps = num_epochs * len(train_dataloader)\n",
    "lr_scheduler = get_scheduler(\n",
    "    name=\"linear\",\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=1000, # A common choice\n",
    "    num_training_steps=num_training_steps\n",
    ")\n",
    "\n",
    "# --- Gradient Scaler for Mixed Precision ---\n",
    "scaler = torch.cuda.amp.GradScaler()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d78d1fde",
   "metadata": {},
   "source": [
    "Main training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "177b4406",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: nan:   0%|          | 6/44490 [19:54:07<147553:46:06, 11941.23s/it]\n",
      "C:\\Users\\dacla\\AppData\\Local\\Temp\\ipykernel_38064\\1864654938.py:14: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "c:\\Users\\dacla\\anaconda3\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:182: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[233], line 19\u001b[0m\n\u001b[0;32m     16\u001b[0m     loss \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mloss\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# Backward pass\u001b[39;00m\n\u001b[1;32m---> 19\u001b[0m scaler\u001b[38;5;241m.\u001b[39mscale(loss)\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# Optimizer step\u001b[39;00m\n\u001b[0;32m     22\u001b[0m scaler\u001b[38;5;241m.\u001b[39mstep(optimizer)\n",
      "File \u001b[1;32mc:\\Users\\dacla\\anaconda3\\Lib\\site-packages\\torch\\_tensor.py:648\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    638\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    639\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    640\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    641\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    646\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    647\u001b[0m     )\n\u001b[1;32m--> 648\u001b[0m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mbackward(\n\u001b[0;32m    649\u001b[0m     \u001b[38;5;28mself\u001b[39m, gradient, retain_graph, create_graph, inputs\u001b[38;5;241m=\u001b[39minputs\n\u001b[0;32m    650\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\dacla\\anaconda3\\Lib\\site-packages\\torch\\autograd\\__init__.py:353\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    348\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    350\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    351\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    352\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 353\u001b[0m _engine_run_backward(\n\u001b[0;32m    354\u001b[0m     tensors,\n\u001b[0;32m    355\u001b[0m     grad_tensors_,\n\u001b[0;32m    356\u001b[0m     retain_graph,\n\u001b[0;32m    357\u001b[0m     create_graph,\n\u001b[0;32m    358\u001b[0m     inputs,\n\u001b[0;32m    359\u001b[0m     allow_unreachable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    360\u001b[0m     accumulate_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    361\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\dacla\\anaconda3\\Lib\\site-packages\\torch\\autograd\\graph.py:824\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    822\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    823\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 824\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    825\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    826\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    827\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    828\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# --- The Training Loop ---\n",
    "progress_bar = tqdm(range(num_training_steps))\n",
    "best_wer = float('inf')\n",
    "output_dir = \".\\\\wav2vec2-custom-loop-best-model\"\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # --- Training Phase ---\n",
    "    model.train()\n",
    "    for batch in train_dataloader:\n",
    "        # Move batch to the correct device\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "\n",
    "        # Use autocast for mixed precision\n",
    "        with torch.cuda.amp.autocast():\n",
    "            outputs = model(**batch)\n",
    "            loss = outputs.loss\n",
    "        \n",
    "        # Backward pass\n",
    "        scaler.scale(loss).backward()\n",
    "        \n",
    "        # Optimizer step\n",
    "        lr_scheduler.step()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        progress_bar.update(1)\n",
    "        progress_bar.set_description(f\"Epoch {epoch+1}, Loss: {loss.item():.4f}\")\n",
    "\n",
    "    # --- Evaluation Phase ---\n",
    "    model.eval()\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "\n",
    "    print(f\"\\n--- Running Evaluation for Epoch {epoch+1} ---\")\n",
    "    for batch in tqdm(eval_dataloader, desc=\"Evaluating\"):\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model(**batch)\n",
    "        \n",
    "        logits = outputs.logits\n",
    "        predicted_ids = torch.argmax(logits, dim=-1)\n",
    "        \n",
    "        all_predictions.extend(predicted_ids.cpu().numpy())\n",
    "        all_labels.extend(batch[\"labels\"].cpu().numpy())\n",
    "        \n",
    "    # Compute WER metric\n",
    "    metric_result = compute_metrics(pred_ids=all_predictions, label_ids=all_labels)\n",
    "    current_wer = metric_result['wer']\n",
    "    print(f\"Epoch {epoch+1} WER: {current_wer:.4f}\")\n",
    "\n",
    "    # --- Save the Best Model ---\n",
    "    if current_wer < best_wer:\n",
    "        best_wer = current_wer\n",
    "        print(f\"New best WER: {best_wer:.4f}. Saving model to {output_dir}\")\n",
    "        model.save_pretrained(output_dir)\n",
    "        processor.save_pretrained(output_dir)\n",
    "\n",
    "print(\"\\n--- Training Finished ---\")\n",
    "print(f\"Best WER achieved: {best_wer:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
