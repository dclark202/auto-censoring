{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "00bce9bb",
   "metadata": {},
   "source": [
    "This notebook is contains the basic training and evaluation loop for fine tuning Whisper. \n",
    "- See whisper-dataset-creation.ipynb to create a dataset from raw audio files\n",
    "- Performance metric functions are found after the main training cycle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27a04fa6",
   "metadata": {},
   "source": [
    "-------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "947ff1e0",
   "metadata": {},
   "source": [
    "Create the Whisper processor\n",
    "- whisper-base is common\n",
    "- Sets device appropriately\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d50b3021",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import WhisperProcessor\n",
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "model_name = \"openai/whisper-base\"\n",
    "language = \"english\" # Change to your dataset's language\n",
    "task = \"transcribe\" # Use \"translate\" if you're trans\"lating to English\n",
    "\n",
    "processor = WhisperProcessor.from_pretrained(model_name, language=language, task=task)\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0de3d6d7",
   "metadata": {},
   "source": [
    "Create Whisper model. \n",
    "- If loading from a fine-tuned checkpoint use pretrained_path variable\n",
    "- create_whisper_model automatically freezes all parameters except for the LM layer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d377374a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import WhisperForConditionalGeneration\n",
    "\n",
    "def create_whisper_model(model_name, device, pretrained_path=False):\n",
    "    if not pretrained_path:\n",
    "        model = WhisperForConditionalGeneration.from_pretrained(model_name)\n",
    "        print(f'Loaded {model_name} on {device}')\n",
    "    else: \n",
    "        model = WhisperForConditionalGeneration.from_pretrained(pretrained_path)\n",
    "        print(f'Loaded {model_name} on {device} from checkpoint {pretrained_path}')\n",
    "\n",
    "    # Send to device\n",
    "    model.to(device)\n",
    "\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    # Except those in the last layer\n",
    "    for param in model.proj_out.parameters():\n",
    "        param.requires_grad = True\n",
    "\n",
    "    # Verify which layers are trainable\n",
    "    print(\"\\nTrainable parameters after freezing:\")\n",
    "    trainable_params = 0\n",
    "    frozen_params = 0\n",
    "    for name, param in model.named_parameters():\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "            print(f\"  - {name} (Trainable, shape: {param.shape})\")\n",
    "        else:\n",
    "            frozen_params += param.numel()\n",
    "            # print(f\"  - {name} (Frozen)\") # Uncomment to see all frozen params\n",
    "\n",
    "    total_params = trainable_params + frozen_params\n",
    "    print(f\"\\nTotal trainable parameters: {trainable_params}\")\n",
    "    print(f\"Total frozen parameters: {frozen_params}\")\n",
    "    print(f\"Total parameters: {total_params}\")\n",
    "    print(f\"Ratio of trained params to total params: {trainable_params / total_params:.4f}\")\n",
    "\n",
    "    return model\n",
    "\n",
    "model = create_whisper_model(model_name, device)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1e8380f",
   "metadata": {},
   "source": [
    "Define DataCollator class for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "18e62126",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForSeq2Seq\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, Dict, List, Union\n",
    "import torch\n",
    "\n",
    "# --- Data Collator ---\n",
    "@dataclass\n",
    "class DataCollatorSpeechSeq2SeqWithPadding:\n",
    "    processor: Any\n",
    "    padding: Union[bool, str] = True\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
    "        # Split inputs and labels since they have to be of different lengths and need different padding methods.\n",
    "        # \"input_features\" for Whisper-based models (vs. \"input_values\" for wav2vec...)\n",
    "        input_features = [{\"input_features\": feature[\"input_features\"]} for feature in features]\n",
    "        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
    "\n",
    "        batch = self.processor.feature_extractor.pad(input_features, \n",
    "                                                     return_tensors=\"pt\",\n",
    "                                                     return_attention_mask=True)\n",
    "        \n",
    "        labels_batch = self.processor.tokenizer.pad(label_features,\n",
    "                                                    padding='longest', \n",
    "                                                    return_tensors=\"pt\",)\n",
    "\n",
    "        # Replace padding with -100 for loss to work correctly correctly\n",
    "        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
    "\n",
    "        batch[\"labels\"] = labels\n",
    "        return batch\n",
    "\n",
    "data_collator = DataCollatorSpeechSeq2SeqWithPadding(processor=processor, padding=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27752e5c",
   "metadata": {},
   "source": [
    "Load an already created dataset. \n",
    "- sample_percentage can be used to downsample the dataset for quick testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c240d898",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "\n",
    "sample_percentage = .1\n",
    "\n",
    "# Load full prepared dataset\n",
    "prepared_dataset_path = 'wer0-dataset-fixed-padding'\n",
    "prepared_datasets = load_from_disk(prepared_dataset_path)\n",
    "print(\"--- Full Prepared Dataset ---\")\n",
    "print(prepared_datasets)\n",
    "\n",
    "# Sample 1% from the training set\n",
    "train_split = prepared_datasets[\"train\"]\n",
    "sampled_train_split = train_split.train_test_split(train_size=sample_percentage, shuffle=True, seed=555)['train'] # We only want the 'train' part of this new split\n",
    "\n",
    "test_split = prepared_datasets[\"test\"]\n",
    "sampled_test_split = test_split.train_test_split(train_size=sample_percentage, shuffle=True, seed=555)['train'] \n",
    "\n",
    "# Overwrite the original splits with the sampled splits\n",
    "prepared_datasets['train'] = sampled_train_split\n",
    "prepared_datasets['test'] = sampled_test_split\n",
    "\n",
    "print(f\"\\n--- Sampled ({sample_percentage*100}%) Dataset ---\")\n",
    "print(prepared_datasets)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "443b335a",
   "metadata": {},
   "source": [
    "Training parameters and dataloaders\n",
    "- Sets learning rate, batch sizes, number of epochs, optimizer and LR scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d562d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from torch.optim import AdamW\n",
    "import re\n",
    "\n",
    "def remove_punctuation(s):\n",
    "    s = re.sub(r'[^a-zA-Z0-9\\s]', '', s)\n",
    "    return s.lower()\n",
    "\n",
    "# Training parameters\n",
    "learning_rate = .001 # Max learning rate\n",
    "train_batch_size = 64 # 64 works with 16GB of VRAM\n",
    "eval_batch_size = 64\n",
    "\n",
    "num_epochs = 20\n",
    "total_steps = len(train_dataloader) * num_epochs\n",
    "\n",
    "# Defined train and test DLs\n",
    "train_dataloader = DataLoader(prepared_datasets[\"train\"], shuffle=True, collate_fn=data_collator, batch_size=train_batch_size)\n",
    "eval_dataloader = DataLoader(prepared_datasets[\"test\"], collate_fn=data_collator, batch_size=eval_batch_size)\n",
    "\n",
    "# Optim and LR scheduler\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "scheduler = CosineAnnealingLR(optimizer, T_max=total_steps, eta_min=.00001)\n",
    "\n",
    "# Forces the model.generate method to transcribe audio interpreted as english\n",
    "forced_decoder_ids = processor.get_decoder_prompt_ids(language=\"en\", task=\"transcribe\")\n",
    "\n",
    "# Directory for best model to be saved\n",
    "output_dir = \".\\\\whisper-ft\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbaf9c10",
   "metadata": {},
   "source": [
    "Main training cycle\n",
    "- Automatically creates and saves a dataframe of the best MER output\n",
    "- Patience counter for early exit of training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "447f352c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import jiwer\n",
    "\n",
    "# use for early stopping of training if no increase in MER is detected\n",
    "patience = 0 \n",
    "best_mer = float('inf')\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # train loop\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    \n",
    "    for batch in tqdm(train_dataloader, desc=f\"(Epoch {epoch+1} / {num_epochs}) Training \"):\n",
    "        # Move batch to device\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "\n",
    "        # Backwards pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    avg_train_loss = train_loss / len(train_dataloader)\n",
    "\n",
    "    # eval loop\n",
    "    model.eval()\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(eval_dataloader, desc=\"Evaluating\"):\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            \n",
    "            # Generate predictions. Note this is different than model.transcribe. \n",
    "            # Use whisper-timestamped for time stamp generation\n",
    "            # model.generate will only generate text transcriptions for 30s of audio\n",
    "            generated_ids = model.generate(input_features=batch[\"input_features\"], \n",
    "                                    attention_mask=batch[\"attention_mask\"], \n",
    "                                    num_beams=3, \n",
    "                                    length_penalty=.8,\n",
    "                                    early_stopping=True,\n",
    "                                    forced_decoder_ids=forced_decoder_ids, # Depricated? \n",
    "                                    pad_token_id=processor.tokenizer.pad_token_id,\n",
    "                                    eos_token_id=processor.tokenizer.eos_token_id\n",
    "                                    )                    \n",
    "            \n",
    "            # Decode predictions\n",
    "            predictions = processor.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "            \n",
    "            # Decode labels, replacing -100 with pad token\n",
    "            labels = batch[\"labels\"].clone()\n",
    "            labels[labels == -100] = processor.tokenizer.pad_token_id\n",
    "            labels_str = processor.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "            all_predictions.extend(predictions)\n",
    "            all_labels.extend(labels_str)\n",
    "\n",
    "    # Compute WER and MER. Since the text field lengths are of varied size, MER is a better metric for correcteness\n",
    "    all_predictions = [remove_punctuation(p) for p in all_predictions]\n",
    "    \n",
    "    wer = jiwer.wer(all_predictions, all_labels)\n",
    "    mer = jiwer.mer(all_predictions, all_labels)\n",
    "\n",
    "    print(f\"Avg training loss: {avg_train_loss:.4f} | Eval. MER: {mer:.5f}, WER: {wer:.5f}\")\n",
    "    print()\n",
    "\n",
    "    # Save the model if it has the best WER so far\n",
    "    if mer < best_mer:\n",
    "        patience = 0 # reset patience counter\n",
    "        \n",
    "        best_mer = mer\n",
    "        print(f\"(!) New best MER: {best_mer}. Saving model...\")\n",
    "        model.save_pretrained(output_dir)\n",
    "        processor.save_pretrained(output_dir)\n",
    "        print(f\"Model saved to {output_dir}\")\n",
    "        print()\n",
    "\n",
    "        ## Create df to analyse the outputs for a best output\n",
    "        to_add = []\n",
    "\n",
    "        for i in range(len(all_predictions)):\n",
    "            pred = all_predictions[i]\n",
    "            actual = all_labels[i]\n",
    "            mer = jiwer.mer(pred, actual)\n",
    "            wer = jiwer.wer(pred, actual)\n",
    "\n",
    "            to_add.append([pred, actual, mer, wer])\n",
    "\n",
    "        df_best = pd.DataFrame(to_add, columns=['predicted', 'actual', 'mer', 'wer'])\n",
    "        df_best.to_csv('best-mer-outputs.csv', index=False)\n",
    "\n",
    "    else: \n",
    "        patience += 1\n",
    "\n",
    "    if patience == 5: \n",
    "        print('No increase in MER detected in 5 rounds, breaking')\n",
    "        break\n",
    "        \n",
    "print(\"\\n--- Training Complete ---\")\n",
    "print(f\"Best MER achieved: {best_mer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e4f9760",
   "metadata": {},
   "source": [
    "--------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb535914",
   "metadata": {},
   "source": [
    "For investigating the outputs of whisper\n",
    "- Load model if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9b5c829",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine tuned model from .\\whisper-ft loaded on cuda\n"
     ]
    }
   ],
   "source": [
    "from transformers import WhisperForConditionalGeneration\n",
    "import pandas as pd\n",
    "import jiwer\n",
    "import torch\n",
    "import re\n",
    "\n",
    "#model = create_whisper_model(model_name, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "922b0ae3",
   "metadata": {},
   "source": [
    "Evaluation cycle only. \n",
    "- Automatically creates a dataframe of the outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5a518d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm \n",
    "\n",
    "forced_decoder_ids = processor.get_decoder_prompt_ids(language=\"en\", task=\"transcribe\")\n",
    "device = 'cuda' if torch.cuda.is_available() else 'gpu'\n",
    "model.eval()\n",
    "\n",
    "all_predictions = []\n",
    "all_norm_preds = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(eval_dataloader, desc=\"Evaluating\"):\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        \n",
    "        # Generate predictions. Note this is different than model.transcribe (which is used for untrained?)\n",
    "        generated_ids = model.generate(input_features=batch[\"input_features\"], \n",
    "                                    attention_mask=batch[\"attention_mask\"], \n",
    "                                    num_beams=3, \n",
    "                                    length_penalty=.8,\n",
    "                                    early_stopping=True,\n",
    "                                    task='transcribe',\n",
    "                                    language='en',\n",
    "                                    pad_token_id=processor.tokenizer.pad_token_id,\n",
    "                                    eos_token_id=processor.tokenizer.eos_token_id\n",
    "                                    )              \n",
    "        \n",
    "        # Decode predictions\n",
    "        predictions = processor.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "\n",
    "        # Decode labels, replacing -100 with pad token\n",
    "        labels = batch[\"labels\"].clone()\n",
    "        labels[labels == -100] = processor.tokenizer.pad_token_id\n",
    "        labels_str = processor.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "        all_predictions.extend(predictions)\n",
    "        all_labels.extend(labels_str)\n",
    "\n",
    "# Create df for easier inspection\n",
    "lst = []\n",
    "\n",
    "for i in range(len(all_predictions)):\n",
    "    pred = all_predictions[i]\n",
    "    pred_norm = remove_punctuation(pred)\n",
    "    actual = all_labels[i]\n",
    "\n",
    "    wer = jiwer.wer(pred_norm, actual)\n",
    "    mer = jiwer.mer(pred_norm, actual)\n",
    "\n",
    "    # print(f'Predicted - {pred}')\n",
    "    # print(f'Actual    - {actual}')\n",
    "    # print(f'Jiwer WER: {wer} | MER: {mer}')\n",
    "    # print()\n",
    "\n",
    "    lst.append([pred, pred_norm, actual, wer, mer])\n",
    "\n",
    "df_wer = pd.DataFrame(l, columns=['Prediction (raw)', 'Prediction (normalized)', 'Actual', 'WER', 'MER'])\n",
    "df_wer.to_csv('wer0-dataset-base-untrained.csv', index=False)\n",
    "\n",
    "# Overall WER and MER scores\n",
    "jwer = jiwer.wer(df_wer['Prediction (normalized)'].tolist(), df_wer['Actual'].tolist())\n",
    "jmer = jiwer.mer(df_wer['Prediction (normalized)'].tolist(), df_wer['Actual'].tolist())\n",
    "\n",
    "#print(f'Evaluate wer: {ev_wer:.5f}')\n",
    "print(f'Overall Jiwer wer: {jwer:.5f} | mer {jmer:.5f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f6d0b58",
   "metadata": {},
   "source": [
    "For investigating the outputs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f87f1bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_wer = pd.read_csv('small-dataset-whisper-trained-outputs.csv')\n",
    "\n",
    "pd.set_option('display.max_colwidth', 60)\n",
    "pd.set_option('display.max_rows', 500)\n",
    "\n",
    "df_wer = df_wer.sort_values(by='WER')\n",
    "\n",
    "df_wer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "030e81a6",
   "metadata": {},
   "source": [
    "------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e1ddab3",
   "metadata": {},
   "source": [
    "Test a model on an audio file. \n",
    "- Audio does not need to be preprocessed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a622cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchaudio\n",
    "\n",
    "def test_transcribe(audio_path):\n",
    "    # Put in evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    # Load audio file\n",
    "    print(f\"Loading audio from: {audio_path}...\")\n",
    "    waveform, sample_rate = torchaudio.load(audio_path)\n",
    "\n",
    "    # Resample if necessary (Whisper expects 16kHz)\n",
    "    if sample_rate != 16000:\n",
    "        print(f\"Resampling audio from {sample_rate}Hz to 16kHz...\")\n",
    "        resampler = torchaudio.transforms.Resample(orig_freq=sample_rate, new_freq=16000)\n",
    "        waveform = resampler(waveform)\n",
    "        sample_rate = 16000 # Update sample rate after resampling\n",
    "\n",
    "    # Ensure mono audio (Whisper expects single channel)\n",
    "    if waveform.shape[0] > 1:\n",
    "        print(\"Converting stereo audio to mono...\")\n",
    "        waveform = waveform.mean(dim=0, keepdim=True) # Average channels to mono\n",
    "\n",
    "    # Convert to numpy array (required by feature_extractor for raw audio)\n",
    "    audio_array = waveform.squeeze().numpy()\n",
    "\n",
    "    # Extract features (Mel spectrogram)\n",
    "    processed_audio = processor.feature_extractor(audio_array, \n",
    "                                                  sampling_rate=sample_rate, \n",
    "                                                  return_tensors=\"pt\",\n",
    "                                                  return_attention_mask=True,\n",
    "                                                  )\n",
    " \n",
    "    input_features = processed_audio.input_features.to(device)\n",
    "    attention_mask = processed_audio.attention_mask.to(device)\n",
    "\n",
    "    print(\"Generating transcription...\")\n",
    "    with torch.no_grad():\n",
    "        generated_ids = model.generate(input_features=input_features, \n",
    "                                       attention_mask=attention_mask,\n",
    "                                       max_new_tokens=400,\n",
    "                                       temperature=0.0,\n",
    "                                       #no_speech_threshold=.3 # Error when using this ?\n",
    "                                       )\n",
    "        \n",
    "    # Create the transcription\n",
    "    transcription = processor.tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "    return transcription"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69c3b685",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and preprocess the audio file\n",
    "audio_path = 'vocals.wav'\n",
    "print(\"\\nTranscription:\\n\", test_transcribe(audio_path))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "auto-censoring",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
