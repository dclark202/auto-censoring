{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b1438a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "# Set directory for audio chunks and load csv with lyrics\n",
    "chunks_dir = \"C:\\\\Users\\\\dacla\\\\Documents\\\\DALI-chunks\"\n",
    "df_chunks = pd.read_csv(\"lyrics-chunks.csv\")\n",
    "\n",
    "df_chunks.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "a3c4ac70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['C:\\\\Users\\\\dacla\\\\Documents\\\\auto-censoring-local\\\\tokenizers\\\\vocab.json',\n",
       " 'C:\\\\Users\\\\dacla\\\\Documents\\\\auto-censoring-local\\\\tokenizers\\\\merges.txt']"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "\n",
    "tokenizer = Tokenizer(BPE(unk_token=\"[UNK]\"))\n",
    "\n",
    "# Split by whitespace\n",
    "tokenizer.pre_tokenizer = Whitespace()\n",
    "\n",
    "# Byte-pair encoding\n",
    "trainer = BpeTrainer(vocab_size=3000, min_frequency=5, special_tokens=[\"[PAD]\", \"[UNK]\", \"|\"])\n",
    "\n",
    "# Text body from the DALI lyrics database\n",
    "file_path = \"C:\\\\Users\\\\dacla\\\\Documents\\\\auto-censoring-local\\\\corpus.txt\"\n",
    "\n",
    "# Train the tokenizer\n",
    "tokenizer.train([file_path], trainer)\n",
    "\n",
    "# And save output\n",
    "token_dir = \"C:\\\\Users\\\\dacla\\\\Documents\\\\auto-censoring-local\\\\tokenizers\"\n",
    "tokenizer.save(f\"{token_dir}\\\\tokenizer.json\")\n",
    "tokenizer.model.save(token_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "dc12ee5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: ['be', 'ans', 'and', 'le', 'gu', 'mes']\n",
      "IDs: [59, 1177, 53, 64, 350, 1227]\n",
      "\n",
      "ID for [PAD]: 0\n",
      "ID for [UNK]: 1\n",
      "ID for '|' (space): 2\n"
     ]
    }
   ],
   "source": [
    "# Test the tokenizer\n",
    "encoded = tokenizer.encode(\"beans and legumes\")\n",
    "print(f\"Tokens: {encoded.tokens}\")\n",
    "print(f\"IDs: {encoded.ids}\")\n",
    "print()\n",
    "\n",
    "\n",
    "print(f\"ID for [PAD]: {tokenizer.token_to_id('[PAD]')}\")\n",
    "print(f\"ID for [UNK]: {tokenizer.token_to_id('[UNK]')}\")\n",
    "print(f\"ID for '|' (space): {tokenizer.token_to_id('|')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "17bf2e2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('.\\\\tokenizers\\\\my_wav2vec2_bpe_tokenizer\\\\tokenizer_config.json',\n",
       " '.\\\\tokenizers\\\\my_wav2vec2_bpe_tokenizer\\\\special_tokens_map.json',\n",
       " '.\\\\tokenizers\\\\my_wav2vec2_bpe_tokenizer\\\\vocab.json',\n",
       " '.\\\\tokenizers\\\\my_wav2vec2_bpe_tokenizer\\\\added_tokens.json')"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import Wav2Vec2CTCTokenizer\n",
    "\n",
    "# Path to the files we just saved\n",
    "vocab_file = \".\\\\tokenizers\\\\vocab.json\"\n",
    "merges_file = \".\\\\tokenizers\\\\merges.txt\"\n",
    "\n",
    "# Load the trained BPE files into the wav2vec2-specific tokenizer class\n",
    "custom_tokenizer = Wav2Vec2CTCTokenizer(\n",
    "    vocab_file=vocab_file,\n",
    "    merges_file=merges_file,\n",
    "    unk_token=\"[UNK]\",\n",
    "    pad_token=\"[PAD]\",\n",
    "    word_delimiter_token=\"|\", # Crucial for wav2vec2\n",
    ")\n",
    "\n",
    "custom_tokenizer.save_pretrained(\".\\\\tokenizers\\\\my_wav2vec2_bpe_tokenizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "09bcf757",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: ['bea', 'ns', 'and', 'leg', 'um', 'es']\n",
      "Encoded IDs: [552, 2787, 53, 2836, 364, 135]\n"
     ]
    }
   ],
   "source": [
    "test_sentence = 'beans and legumes'\n",
    "\n",
    "tokens = custom_tokenizer.tokenize(test_sentence)\n",
    "print(\"Tokens:\", tokens)\n",
    "\n",
    "encoded = custom_tokenizer(test_sentence).input_ids\n",
    "print(\"Encoded IDs:\", encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "826c9b38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processor created and saved.\n"
     ]
    }
   ],
   "source": [
    "from transformers import Wav2Vec2FeatureExtractor, Wav2Vec2Processor\n",
    "\n",
    "# 1. Load your custom tokenizer\n",
    "tokenizer = Wav2Vec2CTCTokenizer.from_pretrained(\".\\\\tokenizers\\\\my_wav2vec2_bpe_tokenizer\")\n",
    "\n",
    "# 2. Create a standard feature extractor\n",
    "feature_extractor = Wav2Vec2FeatureExtractor(\n",
    "    feature_size=1,\n",
    "    sampling_rate=16000,\n",
    "    padding_value=0.0,\n",
    "    do_normalize=True,\n",
    "    return_attention_mask=False\n",
    ")\n",
    "\n",
    "# 3. Bundle them into a processor\n",
    "processor = Wav2Vec2Processor(feature_extractor=feature_extractor, tokenizer=tokenizer)\n",
    "\n",
    "# Save the processor for easy loading later\n",
    "processor.save_pretrained(\"my_wav2vec2_processor\")\n",
    "print(\"Processor created and saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "25653aa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/wav2vec2-base and are newly initialized: ['lm_head.bias', 'lm_head.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Old LM Head: Linear(in_features=768, out_features=32, bias=True)\n",
      "Old Vocab Size (from config): 32\n",
      "--------------------\n",
      "New LM Head: Linear(in_features=768, out_features=3000, bias=True)\n",
      "New Vocab Size (from config): 3000\n",
      "Model loaded and output layer resized for the new vocabulary.\n"
     ]
    }
   ],
   "source": [
    "from transformers import Wav2Vec2ForCTC\n",
    "\n",
    "model_name = \"facebook/wav2vec2-base\"\n",
    "model = Wav2Vec2ForCTC.from_pretrained(model_name)\n",
    "\n",
    "print(f\"Old LM Head: {model.lm_head}\")\n",
    "print(f\"Old Vocab Size (from config): {model.config.vocab_size}\")\n",
    "\n",
    "# 3. Manually replace the LM head\n",
    "# Get the model's hidden size\n",
    "hidden_size = model.config.hidden_size\n",
    "\n",
    "# Create a new linear layer with the correct dimensions\n",
    "new_lm_head = torch.nn.Linear(hidden_size, new_vocab_size)\n",
    "\n",
    "# Replace the old lm_head with the new one\n",
    "model.lm_head = new_lm_head\n",
    "\n",
    "# 4. VERY IMPORTANT: Update the model's config to reflect the new vocab size\n",
    "model.config.vocab_size = new_vocab_size\n",
    "model.config.pad_token_id = processor.tokenizer.pad_token_id\n",
    "\n",
    "print(\"-\" * 20)\n",
    "print(f\"New LM Head: {model.lm_head}\")\n",
    "print(f\"New Vocab Size (from config): {model.config.vocab_size}\")\n",
    "\n",
    "# It's a good practice to freeze the feature extractor part of the model\n",
    "# during the initial phase of fine-tuning.\n",
    "model.freeze_feature_encoder()\n",
    "\n",
    "print(\"Model loaded and output layer resized for the new vocabulary.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8bca685",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
