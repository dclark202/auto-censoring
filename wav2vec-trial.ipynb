{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "75c5753e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.7.1+cu126\n",
      "12.6\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'flash_attn'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(torch\u001b[38;5;241m.\u001b[39mversion\u001b[38;5;241m.\u001b[39mcuda)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mflash_attn\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFlash Attention installed successfully!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(flash_attn\u001b[38;5;241m.\u001b[39m__version__)\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'flash_attn'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)      # e.g., '2.3.0+cu121' means PyTorch 2.3.0 built with CUDA 12.1\n",
    "print(torch.version.cuda)\n",
    "\n",
    "import torch\n",
    "import flash_attn\n",
    "print(\"Flash Attention installed successfully!\")\n",
    "print(flash_attn.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0af8a673",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "expected str, bytes or os.PathLike object, not NoneType",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[26], line 9\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Initializing a model (with random weights) from the facebook/wav2vec2-bert-rel-pos-large style configuration\u001b[39;00m\n\u001b[0;32m      7\u001b[0m model \u001b[38;5;241m=\u001b[39m Wav2Vec2BertModel(configuration)\n\u001b[1;32m----> 9\u001b[0m processor \u001b[38;5;241m=\u001b[39m Wav2Vec2BertProcessor\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfacebook/w2v-bert-2.0\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# Accessing the model configuration\u001b[39;00m\n\u001b[0;32m     12\u001b[0m configuration \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mconfig\n",
      "File \u001b[1;32mc:\\Users\\dacla\\anaconda3\\Lib\\site-packages\\transformers\\models\\wav2vec2_bert\\processing_wav2vec2_bert.py:56\u001b[0m, in \u001b[0;36mWav2Vec2BertProcessor.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfrom_pretrained\u001b[39m(\u001b[38;5;28mcls\u001b[39m, pretrained_model_name_or_path, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m     55\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 56\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mfrom_pretrained(pretrained_model_name_or_path, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     57\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m:\n\u001b[0;32m     58\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m     59\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoading a tokenizer inside \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m from a config that does not\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     60\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m include a `tokenizer_class` attribute is deprecated and will be \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     64\u001b[0m             \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[0;32m     65\u001b[0m         )\n",
      "File \u001b[1;32mc:\\Users\\dacla\\anaconda3\\Lib\\site-packages\\transformers\\processing_utils.py:1185\u001b[0m, in \u001b[0;36mProcessorMixin.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, cache_dir, force_download, local_files_only, token, revision, **kwargs)\u001b[0m\n\u001b[0;32m   1182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m token \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1183\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m token\n\u001b[1;32m-> 1185\u001b[0m args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_get_arguments_from_pretrained(pretrained_model_name_or_path, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1186\u001b[0m processor_dict, kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mget_processor_dict(pretrained_model_name_or_path, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1187\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mfrom_args_and_dict(args, processor_dict, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\dacla\\anaconda3\\Lib\\site-packages\\transformers\\processing_utils.py:1248\u001b[0m, in \u001b[0;36mProcessorMixin._get_arguments_from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[0;32m   1245\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1246\u001b[0m         attribute_class \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mget_possibly_dynamic_module(class_name)\n\u001b[1;32m-> 1248\u001b[0m     args\u001b[38;5;241m.\u001b[39mappend(attribute_class\u001b[38;5;241m.\u001b[39mfrom_pretrained(pretrained_model_name_or_path, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs))\n\u001b[0;32m   1249\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m args\n",
      "File \u001b[1;32mc:\\Users\\dacla\\anaconda3\\Lib\\site-packages\\transformers\\models\\auto\\tokenization_auto.py:1035\u001b[0m, in \u001b[0;36mAutoTokenizer.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[0m\n\u001b[0;32m   1033\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1034\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m tokenizer_class_py \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1035\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer_class_py\u001b[38;5;241m.\u001b[39mfrom_pretrained(pretrained_model_name_or_path, \u001b[38;5;241m*\u001b[39minputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1036\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1037\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1038\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis tokenizer cannot be instantiated. Please make sure you have `sentencepiece` installed \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1039\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124min order to use this tokenizer.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1040\u001b[0m         )\n",
      "File \u001b[1;32mc:\\Users\\dacla\\anaconda3\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:2025\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, cache_dir, force_download, local_files_only, token, revision, trust_remote_code, *init_inputs, **kwargs)\u001b[0m\n\u001b[0;32m   2022\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   2023\u001b[0m         logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloading file \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m from cache at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresolved_vocab_files[file_id]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 2025\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_from_pretrained(\n\u001b[0;32m   2026\u001b[0m     resolved_vocab_files,\n\u001b[0;32m   2027\u001b[0m     pretrained_model_name_or_path,\n\u001b[0;32m   2028\u001b[0m     init_configuration,\n\u001b[0;32m   2029\u001b[0m     \u001b[38;5;241m*\u001b[39minit_inputs,\n\u001b[0;32m   2030\u001b[0m     token\u001b[38;5;241m=\u001b[39mtoken,\n\u001b[0;32m   2031\u001b[0m     cache_dir\u001b[38;5;241m=\u001b[39mcache_dir,\n\u001b[0;32m   2032\u001b[0m     local_files_only\u001b[38;5;241m=\u001b[39mlocal_files_only,\n\u001b[0;32m   2033\u001b[0m     _commit_hash\u001b[38;5;241m=\u001b[39mcommit_hash,\n\u001b[0;32m   2034\u001b[0m     _is_local\u001b[38;5;241m=\u001b[39mis_local,\n\u001b[0;32m   2035\u001b[0m     trust_remote_code\u001b[38;5;241m=\u001b[39mtrust_remote_code,\n\u001b[0;32m   2036\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   2037\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\dacla\\anaconda3\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:2278\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase._from_pretrained\u001b[1;34m(cls, resolved_vocab_files, pretrained_model_name_or_path, init_configuration, token, cache_dir, local_files_only, _commit_hash, _is_local, trust_remote_code, *init_inputs, **kwargs)\u001b[0m\n\u001b[0;32m   2276\u001b[0m \u001b[38;5;66;03m# Instantiate the tokenizer.\u001b[39;00m\n\u001b[0;32m   2277\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 2278\u001b[0m     tokenizer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m(\u001b[38;5;241m*\u001b[39minit_inputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minit_kwargs)\n\u001b[0;32m   2279\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m import_protobuf_decode_error():\n\u001b[0;32m   2280\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\n\u001b[0;32m   2281\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnable to load tokenizer model from SPM, loading from TikToken will be attempted instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2282\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(Google protobuf error: Tried to load SPM model with non-SPM vocab file).\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   2283\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\dacla\\anaconda3\\Lib\\site-packages\\transformers\\models\\wav2vec2\\tokenization_wav2vec2.py:168\u001b[0m, in \u001b[0;36mWav2Vec2CTCTokenizer.__init__\u001b[1;34m(self, vocab_file, bos_token, eos_token, unk_token, pad_token, word_delimiter_token, replace_word_delimiter_char, do_lower_case, target_lang, **kwargs)\u001b[0m\n\u001b[0;32m    165\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreplace_word_delimiter_char \u001b[38;5;241m=\u001b[39m replace_word_delimiter_char\n\u001b[0;32m    166\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_lang \u001b[38;5;241m=\u001b[39m target_lang\n\u001b[1;32m--> 168\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(vocab_file, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m vocab_handle:\n\u001b[0;32m    169\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvocab \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mload(vocab_handle)\n\u001b[0;32m    171\u001b[0m \u001b[38;5;66;03m# if target lang is defined vocab must be a nested dict\u001b[39;00m\n\u001b[0;32m    172\u001b[0m \u001b[38;5;66;03m# with each target lang being one vocabulary\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: expected str, bytes or os.PathLike object, not NoneType"
     ]
    }
   ],
   "source": [
    "from transformers import Wav2Vec2BertConfig, Wav2Vec2BertModel, Wav2Vec2BertProcessor\n",
    "\n",
    "# Initializing a Wav2Vec2Bert facebook/wav2vec2-bert-rel-pos-large style configuration\n",
    "configuration = Wav2Vec2BertConfig()\n",
    "\n",
    "# Initializing a model (with random weights) from the facebook/wav2vec2-bert-rel-pos-large style configuration\n",
    "model = Wav2Vec2BertModel(configuration)\n",
    "\n",
    "processor = Wav2Vec2BertProcessor.from_pretrained(\"facebook/w2v-bert-2.0\")\n",
    "\n",
    "# Accessing the model configuration\n",
    "configuration = model.config\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76caff48",
   "metadata": {},
   "source": [
    "START HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5e57d6ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized: ['wav2vec2.masked_spec_embed']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "(!) Loaded facebook/wav2vec2-base-960h on cuda\n"
     ]
    }
   ],
   "source": [
    "from transformers import Wav2Vec2Processor, Wav2Vec2ForCTC, Wav2Vec2Tokenizer\n",
    "from transformers import Wav2Vec2BertProcessor, Wav2Vec2BertForCTC\n",
    "import torch\n",
    "import torchaudio\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Choose a pre-trained model. Popular choices include:\n",
    "# \"facebook/wav2vec2-base-960h\"\n",
    "# \"facebook/wav2vec2-large-960h\"\n",
    "# \"facebook/wav2vec2-large-xlsr-53\" (for multilingual models)\n",
    "model_name = \"facebook/wav2vec2-base-960h\"\n",
    "\n",
    "# Load the processor (tokenizer and feature extractor)\n",
    "processor = Wav2Vec2Processor.from_pretrained(model_name)\n",
    "\n",
    "# Load the model (Wav2Vec2ForCTC is commonly used for ASR)\n",
    "model = Wav2Vec2ForCTC.from_pretrained(model_name)\n",
    "\n",
    "# This is standard\n",
    "target_sample_rate = 16000\n",
    "\n",
    "# Move to gpu if avail\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = model.to(device)\n",
    "\n",
    "# Train only the classification layer\n",
    "model.freeze_feature_encoder()\n",
    "\n",
    "print(f\"\\n(!) Loaded {model_name} on {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07fb10d3",
   "metadata": {},
   "source": [
    "Do a sample evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "34225bfe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk>'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def process_and_transcribe(audio_path, target_sample_rate=16000):\n",
    "    speech_array, sampling_rate = torchaudio.load(audio_path)\n",
    "\n",
    "    # Turn into mono\n",
    "    if speech_array.ndim > 1:\n",
    "        speech_array = speech_array.mean(dim=0, keepdim=True) \n",
    "\n",
    "    # Remove outer dim of speech array\n",
    "    speech_array = speech_array.squeeze(0) \n",
    "\n",
    "    if sampling_rate != target_sample_rate:\n",
    "        resampler = torchaudio.transforms.Resample(orig_freq=sampling_rate, new_freq=target_sample_rate)\n",
    "        speech_array = resampler(speech_array)\n",
    "\n",
    "    input_features = processor(\n",
    "        speech_array.numpy(), # Wav2Vec2 models typically expect numpy arrays\n",
    "        sampling_rate=target_sample_rate,\n",
    "        return_tensors=\"pt\"\n",
    "    ).input_values.to(device)\n",
    "\n",
    "    with torch.no_grad(): # Disable gradient calculations to save memory and speed up inference\n",
    "        logits = model(input_features).logits\n",
    "        \n",
    "    predicted_ids = torch.argmax(logits, dim=-1)\n",
    "    transcription = processor.batch_decode(predicted_ids)[0]\n",
    "\n",
    "    return transcription.lower()\n",
    "\n",
    "audio_path = \"C:\\\\Users\\\\dacla\\\\Documents\\\\auto-censoring-local\\\\separated\\\\mdx_extra\\\\clint\\\\vocals.mp3\"\n",
    "process_and_transcribe(audio_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0961116e",
   "metadata": {},
   "source": [
    "Another trial with a DALI song\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16578e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydub import AudioSegment\n",
    "import demucs.separate\n",
    "\n",
    "def extract_portions(input_audio_path, times):\n",
    "    # Get path names for inputs\n",
    "    root, ext = os.path.splitext(os.path.basename(input_audio_path))\n",
    "    dir = os.path.dirname(input_audio_path)\n",
    "\n",
    "    # Define path names for demucs\n",
    "    demucs_path = os.path.abspath(f\"separated/mdx_extra/{root}\")\n",
    "    vocals_path = os.path.join(demucs_path, \"vocals.wav\")\n",
    "    no_vocals_path = os.path.join(demucs_path, \"no_vocals.wav\")\n",
    "\n",
    "    # Do not rerun if not needed\n",
    "    if not os.path.isfile(vocals_path):\n",
    "        demucs.separate.main([\"--two-stems\", \"vocals\", \"-n\", \"mdx_extra\", input_audio_path])\n",
    "\n",
    "    audio = AudioSegment.from_file(vocals_path)\n",
    "\n",
    "    segment_names = []\n",
    "\n",
    "    for i in range(len(times)):\n",
    "        start = times[i][0] * 1000\n",
    "        end = times[i][1] * 1000\n",
    "\n",
    "        # Split the portion that contains vocals\n",
    "        portion = audio[start:end]\n",
    "\n",
    "        # Keep in order\n",
    "        output_path = os.path.join(dir, f\"{root}-chunk-{i}.mp3\")\n",
    "        portion.export(output_path, format=\"mp3\")\n",
    "\n",
    "        segment_names.append(output_path)\n",
    "    \n",
    "    # Delete splits\n",
    "    os.remove(vocals_path)\n",
    "    os.remove(no_vocals_path)\n",
    "    os.rmdir(demucs_path)\n",
    "\n",
    "    return segment_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5f4b4ad3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected model is a bag of 4 models. You will see that many progress bars per track.\n",
      "Separated tracks will be stored in C:\\Users\\dacla\\Documents\\auto-censoring-local\\separated\\mdx_extra\n",
      "Separating track c:\\Users\\dacla\\Documents\\DALI-audio\\001940b614eb43f4a0c826d49a67d66d\\woman-in-love.mp3\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 18\u001b[0m\n\u001b[0;32m     15\u001b[0m     end \u001b[38;5;241m=\u001b[39m row[\u001b[38;5;241m3\u001b[39m]\n\u001b[0;32m     16\u001b[0m     times\u001b[38;5;241m.\u001b[39mappend([start, end])\n\u001b[1;32m---> 18\u001b[0m extract_portions(audio_path, times)\n",
      "Cell \u001b[1;32mIn[2], line 16\u001b[0m, in \u001b[0;36mextract_portions\u001b[1;34m(input_audio_path, times)\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# Do not rerun if not needed\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misfile(vocals_path):\n\u001b[1;32m---> 16\u001b[0m     demucs\u001b[38;5;241m.\u001b[39mseparate\u001b[38;5;241m.\u001b[39mmain([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m--two-stems\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvocals\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m-n\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmdx_extra\u001b[39m\u001b[38;5;124m\"\u001b[39m, input_audio_path])\n\u001b[0;32m     18\u001b[0m audio \u001b[38;5;241m=\u001b[39m AudioSegment\u001b[38;5;241m.\u001b[39mfrom_file(vocals_path)\n\u001b[0;32m     20\u001b[0m segment_names \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[1;32mc:\\Users\\dacla\\anaconda3\\Lib\\site-packages\\demucs\\separate.py:173\u001b[0m, in \u001b[0;36mmain\u001b[1;34m(opts)\u001b[0m\n\u001b[0;32m    171\u001b[0m wav \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m ref\u001b[38;5;241m.\u001b[39mmean()\n\u001b[0;32m    172\u001b[0m wav \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m=\u001b[39m ref\u001b[38;5;241m.\u001b[39mstd()\n\u001b[1;32m--> 173\u001b[0m sources \u001b[38;5;241m=\u001b[39m apply_model(model, wav[\u001b[38;5;28;01mNone\u001b[39;00m], device\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39mdevice, shifts\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39mshifts,\n\u001b[0;32m    174\u001b[0m                       split\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39msplit, overlap\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39moverlap, progress\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    175\u001b[0m                       num_workers\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39mjobs, segment\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39msegment)[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    176\u001b[0m sources \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m=\u001b[39m ref\u001b[38;5;241m.\u001b[39mstd()\n\u001b[0;32m    177\u001b[0m sources \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m ref\u001b[38;5;241m.\u001b[39mmean()\n",
      "File \u001b[1;32mc:\\Users\\dacla\\anaconda3\\Lib\\site-packages\\demucs\\apply.py:187\u001b[0m, in \u001b[0;36mapply_model\u001b[1;34m(model, mix, shifts, split, overlap, transition_power, progress, device, num_workers, segment, pool)\u001b[0m\n\u001b[0;32m    185\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sub_model, model_weights \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(model\u001b[38;5;241m.\u001b[39mmodels, model\u001b[38;5;241m.\u001b[39mweights):\n\u001b[0;32m    186\u001b[0m     original_model_device \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28miter\u001b[39m(sub_model\u001b[38;5;241m.\u001b[39mparameters()))\u001b[38;5;241m.\u001b[39mdevice\n\u001b[1;32m--> 187\u001b[0m     sub_model\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m    189\u001b[0m     out \u001b[38;5;241m=\u001b[39m apply_model(sub_model, mix, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    190\u001b[0m     sub_model\u001b[38;5;241m.\u001b[39mto(original_model_device)\n",
      "File \u001b[1;32mc:\\Users\\dacla\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1355\u001b[0m, in \u001b[0;36mModule.to\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1352\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1353\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[1;32m-> 1355\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_apply(convert)\n",
      "File \u001b[1;32mc:\\Users\\dacla\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:915\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn, recurse)\u001b[0m\n\u001b[0;32m    913\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[0;32m    914\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[1;32m--> 915\u001b[0m         module\u001b[38;5;241m.\u001b[39m_apply(fn)\n\u001b[0;32m    917\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[0;32m    918\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[0;32m    919\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[0;32m    920\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    925\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[0;32m    926\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\dacla\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:915\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn, recurse)\u001b[0m\n\u001b[0;32m    913\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[0;32m    914\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[1;32m--> 915\u001b[0m         module\u001b[38;5;241m.\u001b[39m_apply(fn)\n\u001b[0;32m    917\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[0;32m    918\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[0;32m    919\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[0;32m    920\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    925\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[0;32m    926\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "    \u001b[1;31m[... skipping similar frames: Module._apply at line 915 (4 times)]\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\dacla\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:915\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn, recurse)\u001b[0m\n\u001b[0;32m    913\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[0;32m    914\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[1;32m--> 915\u001b[0m         module\u001b[38;5;241m.\u001b[39m_apply(fn)\n\u001b[0;32m    917\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[0;32m    918\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[0;32m    919\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[0;32m    920\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    925\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[0;32m    926\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\dacla\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:942\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn, recurse)\u001b[0m\n\u001b[0;32m    938\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[0;32m    939\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[0;32m    940\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[0;32m    941\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m--> 942\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m fn(param)\n\u001b[0;32m    943\u001b[0m p_should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[0;32m    945\u001b[0m \u001b[38;5;66;03m# subclasses may have multiple child tensors so we need to use swap_tensors\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\dacla\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1341\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[1;34m(t)\u001b[0m\n\u001b[0;32m   1334\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[0;32m   1335\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(\n\u001b[0;32m   1336\u001b[0m             device,\n\u001b[0;32m   1337\u001b[0m             dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1338\u001b[0m             non_blocking,\n\u001b[0;32m   1339\u001b[0m             memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format,\n\u001b[0;32m   1340\u001b[0m         )\n\u001b[1;32m-> 1341\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(\n\u001b[0;32m   1342\u001b[0m         device,\n\u001b[0;32m   1343\u001b[0m         dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1344\u001b[0m         non_blocking,\n\u001b[0;32m   1345\u001b[0m     )\n\u001b[0;32m   1346\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m   1347\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e) \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot copy out of meta tensor; no data!\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "dali_dir = os.path.abspath(f\"../DALI-audio\")\n",
    "\n",
    "for id in os.listdir(dali_dir)[:10]:\n",
    "    for file in os.listdir(os.path.join(dali_dir, id)):\n",
    "        root, ext = os.path.splitext(file)\n",
    "\n",
    "        if ext == '.mp3':\n",
    "            audio_path = os.path.join(os.path.join(dali_dir, id), file)\n",
    "\n",
    "    df = pd.read_csv(f\"../DALI-audio/{id}/lyrics-paragraphs.csv\")\n",
    "\n",
    "    times = []\n",
    "    for row in df.itertuples():\n",
    "        start = row[2]\n",
    "        end = row[3]\n",
    "        times.append([start, end])\n",
    "\n",
    "    extract_portions(audio_path, times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "846915d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "007c0152242340008ff45781a9b08546\n",
      "Selected model is a bag of 4 models. You will see that many progress bars per track.\n",
      "Separated tracks will be stored in C:\\Users\\dacla\\Documents\\auto-censoring-local\\separated\\mdx_extra\n",
      "Separating track c:\\Users\\dacla\\Documents\\DALI-audio\\007c0152242340008ff45781a9b08546\\wildfire.mp3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████| 231.0/231.0 [00:02<00:00, 102.45seconds/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████| 231.0/231.0 [00:02<00:00, 100.21seconds/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████| 231.0/231.0 [00:02<00:00, 114.96seconds/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████| 231.0/231.0 [00:01<00:00, 115.84seconds/s]\n"
     ]
    }
   ],
   "source": [
    "dali_dir = os.path.abspath(f\"../DALI-audio\")\n",
    "\n",
    "# Just select the first song\n",
    "id = os.listdir(dali_dir)[2]\n",
    "print(id)\n",
    "\n",
    "for file in os.listdir(os.path.join(dali_dir, id)):\n",
    "    root, ext = os.path.splitext(file)\n",
    "\n",
    "    if ext == '.mp3':\n",
    "        audio_path = os.path.join(os.path.join(dali_dir, id), file)\n",
    "\n",
    "df = pd.read_csv(f\"../DALI-audio/{id}/lyrics-paragraphs.csv\")\n",
    "\n",
    "times = []\n",
    "for row in df.itertuples():\n",
    "    start = row[2]\n",
    "    end = row[3]\n",
    "    times.append([start, end])\n",
    "\n",
    "segment_names = extract_portions(audio_path, times)\n",
    "lyrics_true = df['words'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "53febf91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<unk><unk>', '<unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk>', '<unk><unk><unk><unk><unk><unk>', '<unk><unk><unk><unk>', '<unk><unk><unk><unk><unk>', '<unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk>', '<unk><unk><unk><unk><unk><unk><unk><unk><unk>', '<unk><unk><unk><unk><unk>', '<unk><unk><unk><unk><unk><unk><unk><unk>']\n",
      "WER score: 1.0\n"
     ]
    }
   ],
   "source": [
    "from evaluate import load\n",
    "\n",
    "n = len(lyrics_true)\n",
    "lyrics_pred = []\n",
    "for i in range(n):\n",
    "    chunk_path = segment_names[i]\n",
    "    pred = process_and_transcribe(audio_path=chunk_path)\n",
    "    lyrics_pred.append(pred.lower())\n",
    "\n",
    "# Word Error Rate metric\n",
    "wer_metric = load(\"wer\")\n",
    "\n",
    "score = wer_metric.compute(predictions=lyrics_pred, references=lyrics_true)\n",
    "\n",
    "print(lyrics_pred)\n",
    "print(f\"WER score: {score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c17ab49c",
   "metadata": {},
   "source": [
    "Now for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6de668de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3, 3, 4, 3, 3, 3, 3, 4, 3, 3, 3, 3, 4, 3, 3, 3, 4, 3, 3, 3, 4, 3, 3, 3,\n",
       "        3, 4, 3, 3, 3, 3, 3, 3, 4, 3, 3, 3, 3, 4, 3, 3, 3, 4, 3, 3, 3, 3, 3, 3,\n",
       "        3, 3, 3, 4, 3, 3, 3, 3, 3, 3, 4, 3, 3, 3, 3, 4, 3, 4, 3, 3, 3, 3, 4, 3,\n",
       "        3, 3, 3, 3, 3, 4, 3, 3, 3, 4, 3, 3, 3, 3, 4, 3, 3, 4, 3, 3, 3, 3, 3, 3,\n",
       "        3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], device='cuda:0')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenize the true lyrics\n",
    "lyrics_true_tok = processor.tokenizer(lyrics_true, padding=True)['input_ids']\n",
    "y = torch.tensor(lyrics_true_tok).to(device)\n",
    "\n",
    "# y is the labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b83ae8d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([9, 471761])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "# process audio\n",
    "def process_audio(audio_path, target_sample_rate=16000):\n",
    "    speech_array, sampling_rate = torchaudio.load(audio_path)\n",
    "\n",
    "    # Turn into mono\n",
    "    if speech_array.ndim > 1:\n",
    "        speech_array = speech_array.mean(dim=0, keepdim=True) \n",
    "\n",
    "    # Remove outer dim of speech array\n",
    "    speech_array = speech_array.squeeze(0) \n",
    "\n",
    "    if sampling_rate != target_sample_rate:\n",
    "        resampler = torchaudio.transforms.Resample(orig_freq=sampling_rate, new_freq=target_sample_rate)\n",
    "        speech_array = resampler(speech_array)\n",
    "\n",
    "    input_features = processor(\n",
    "        speech_array.numpy(), # Wav2Vec2 models typically expect numpy arrays\n",
    "        sampling_rate=target_sample_rate,\n",
    "        return_tensors=\"pt\"\n",
    "    ).input_values.to(device)\n",
    "\n",
    "    return input_features\n",
    "\n",
    "\n",
    "processed_segments = []\n",
    "\n",
    "for audio_path in segment_names:\n",
    "    proc = process_audio(audio_path)\n",
    "    processed_segments.append(proc.squeeze(0))\n",
    "\n",
    "# X is the input to our training loop\n",
    "X = pad_sequence(processed_segments, batch_first=True)\n",
    "\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85fa1ca3",
   "metadata": {},
   "source": [
    "Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a130a389",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: loss 1229.5851\n",
      "Epoch 2: loss 16281.6602\n",
      "Epoch 3: loss 7500.6543\n",
      "Epoch 4: loss 13785.4355\n",
      "Epoch 5: loss 4012.8577\n",
      "Epoch 6: loss 5635.2363\n",
      "Epoch 7: loss 478.2917\n",
      "Epoch 8: loss 826.8695\n",
      "Epoch 9: loss 14415.0410\n",
      "Epoch 10: loss 3383.9194\n",
      "Epoch 11: loss 5849.9966\n",
      "Epoch 12: loss 11001.1699\n",
      "Epoch 13: loss 7768.5981\n",
      "Epoch 14: loss 3361.2402\n",
      "Epoch 15: loss 6864.5527\n",
      "Epoch 16: loss 2723.0771\n",
      "Epoch 17: loss 1344.1112\n",
      "Epoch 18: loss -76.8533\n",
      "Epoch 19: loss 1971.2827\n",
      "Epoch 20: loss 974.2751\n"
     ]
    }
   ],
   "source": [
    "import torch.optim\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=.001)\n",
    "#scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=500, gamma=0.1)\n",
    "\n",
    "model.train()\n",
    "\n",
    "for epoch in range(20):\n",
    "    outputs = model(X, labels=y)  \n",
    "    loss = outputs.loss\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    #scheduler.step()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    print(f'Epoch {epoch + 1}: loss {loss.item():.4f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
